---
layout: post
title: "Continuous Macro Actions"
tags: [Research, RL]
---
## Macro-Actions Quick Overview

## What is a Macro-Action
A macro-action is essentially a high-level action. The main purpose of a macro-action is for robots to have the ability to asynchronously learn to collaborate with another through a multi-agent reinforcement learning lens. It is more formally defined in the Continuous macro-action as a Mac-DecPOMDP section.

## Continuous Macro Actions 
Writing Research paper on extending Macro-Actions to the continuous case. The main idea is that currently there is no formal definition for a continuous macro-action. The purpose of the continuous macro-action is similar to the reason for a continuous action; scalability. Let's take an example of a mars rover exploring scientific points of interests. Instead of the discrete macro-action "go-to-point-A" it would be "go-to-location-(coordinates)". This means that the macro-action is now a parameter (coordinates) that can be learned. The reason why we need continuous macro-action is for more realistic and refined control. 

Let's have an example with two rovers in a multi-agent scenario. We can assume they are homogenous agents with their macro-action/action space being the same. In this case, if both agents take the action to "go-to-point-A" and point A is this predefined position, it would collide with each other as they are trying to go to the same point and collect the same data. In the continuous case, they would be able to figure out they need to be side by side with one another or on opposite sides. In this case, they could both be at point A without collision of duplicate observations from the rover/agent's perspective. More to be explained in later section. 

## Notation and Function Definitions
These are the commmon functions and notations that you would probably see in a macro-action paper. 

-   $Q^{\theta_i}_{\phi_i}(h_i, m_i)$ is the decentralized critic
-   $Q^{\vec{\Psi}}_{\phi}(\vec{h}, \vec{m})$ is the centralized critic
-   $\Psi_{\theta_i}(m_i \mid h_i)$ is the macro-action-based policy and is the individual actor
-   $\Psi_{\theta}(\vec{m} \mid \vec{h})$ is the centralized actor or joint macro-action-based policy
-   $V_{\mathbf{w}_i}^{\Psi_{\theta_i}}(h_i)$ is the local history value function or critic
-   $V_{\mathbf{w}}^{\Psi_{\theta}}(\vec{h})$ is the centralized history value function or critic
-   $V_{\mathbf{w_i}}^{\vec{\Psi}_{\theta_i}}(\vec{h_i})$ is the separate centralized critic used in MAPPO
-   $r_i^c$ is the cumulative-discounted reward of the macro-action taking $\tau_{mi}$ time steps from beginning.
-   $Q_{\phi_i}(h_i, m_i)$ is the individual macro-action-value function
-   $Q_{\phi_i}(\vec{h}_i, \vec{m}_i)$ is the joint macro-action value function
-   $x$ represents the available centralized information
-   $\vec{A}$ advantage calculated using centralized information using GAE
-   $\alpha$ is the positive coefficient for clipping function
-   $\beta$ is the negative coefficient for the clipping function

---

## Parameterized/Continuous Macro-Action

**Reason/Motivation:** It is mainly for scalability. If we use discrete macro-actions, and it is not parameterized, then for any large grid world, we would have a problem because it would be length × width amount of macro-actions to get to every point on the grid (same way of why we use continuous actions). With continuous macro-actions, we could solve that because we can learn the macro action that gets to where we want without having to store all the discrete macro-actions. This could translate to real-world control problems with robotics. There are many examples out there already on robots learning high-level actions in a hierarchical manner so they are able to collaborate with one another. What we are doing here is formalizing that in a MARL manner as a multi-agent way to do multi-robot collaboration in the real world.

Continuous Macro-Actions can have the underlying policy running discrete or continuous primitive actions. For the purpose of being succint, we will just define the high-level policy in the continuous space but the lower-level policy will still be discrete. It is also possible to define both the high and low level policies to be continuous. In essence, high level is continuous but the lower level can be discrete or continuous.

Also real world robotics do not use discrete actions as robots can’t teleport to a place. It would need to move in continuous time to get to a certain area. We can form it in a way that looks discrete but the underlying actions are still continuous.

We can learn the termination condition by adding it to the MAPPO. Since the termination condition is learned, we can have macro-actions that can be more finely controlled. Other than the policy being continuous instead of discrete, the equations would remain the same.

#### Continuous Macro-Actions:

We assume that every continuous macro-action
$$ m \in \mathcal{M} \subset \mathbb{R}^d $$
can be *deterministically* decomposed into a sequence of primitive actions in each agent's primitive action set $\mathcal{A}_i \subset \mathbb{R}^k$. Concretely, there exists $(a_0, a_1, \dots, a_{\tau-1})$, where each
$$ a_t \in A = \prod_{i \in I} A_i $$
In this case, $\tau$ is the length of the macro-action.

1.  There is a deterministic map
    $$ \phi : A^\tau \longrightarrow \mathcal{M}, \qquad m = \phi(a_0, a_1, \dots, a_{\tau-1}) $$
2.  During execution of $m$, at each underlying step $t=0,\dots,\tau-1$, the agent executes a discrete primitive action based on the local primitive observation history $H^A_i$.
3.  The macro-observation $z$ seen at macro-termination can itself be viewed as a function of the primitive observations: $\psi: \Omega^\tau \to \mathcal{Z}$.

#### Continuous macro-action as a Mac-DecPOMDP:
A MacDec-POMDP \cite{amato_planning_nodate,amato_planning_2014} is defined as the tuple:

$$\langle I, S, A, M, \Omega, \zeta, T, R, O, Z, \mathbb{H}, \gamma \rangle$$

The tuple $\langle I, S, A, \Omega, O, R \rangle$ in MacDec-POMDP are from the definition of Dec-POMDP.
$I$ represents a set of agents;
$S$ defines the environmental state space;
$A = \prod_{i \in I} A_i$ denotes the combined primitive-action space, from each agent's individual primitive-action set $A_i$;
$M = \prod_{i \in I} M_i$ indicates the joint macro-action space, comprising each agent's macro-action space $M_i$;
$\Omega = \prod_{i \in I} \Omega_i$ describes the joint primitive-observation space, combining each agent's primitive-observation set $\Omega_i$;
$\zeta = \prod_{i \in I} \zeta_i$ represents the joint macro-observation space, which combines each agent's macro-observation space $\zeta_i$;
$T(s, \vec{a}, s') = P(s' \mid s, \vec{a})$ explains the environment transition dynamics;
$R(s, \vec{a})$ serves as the global reward function.

When using macro-actions, each agent independently selects a macro-action based on the high level policy and collects a macro-observation. The objective of solving MacDec-POMDPs with a finite horizon is finding a joint high-level policy $\vec{\Psi} = \prod_{i \in I} \Psi_i$ that maximizes the value:

$$V^{\vec{\Psi}}(s_{(0)}) = \mathbb{E}\left[\sum_{t=0}^{\mathbb{H}-1} \gamma^t r\left(s_{(t)}, \vec{a}_{(t)}\right) \mid s_{(0)}, \vec{\pi}, \vec{\Psi}\right]$$

where $\gamma \in [0,1]$ is the discount, and $\mathbb{H}$ is the number of time steps until the problem terminates. 

The idea is that it will randomly choose an ending point at some time step $t | h$. Learning policy over m as well as policy over b. Or say that previously it is predefined but now we assume it is not. Write as two seperate functions or as one thing. Answer when do I learn the termination function and when do I learn the other one? What would be the value if i stopped vs continued.

When the option terminates (at primitive step $\tau_i$), there will be a new *macro-observation* $z^i$ via
$$ Z_i\bigl(z^i\mid m^i,\,s_{\tau_i}\bigr) = P\bigl(z^i\mid h^i_{\tau_i},m^i\bigr), $$
and its high-level macro-action observation history $h^i\in H^M_i$.

Under these definitions, we will have the high-level policy
$$ \Psi_i\;(m^i\mid h^i)\,,\quad m^i\in M_i $$
becoming the density over options.

---

## Problems?

-   When objectives suddenly change like the box suddenly no longer exist in the preset position so it needs to cancel the macro-action and choose to go elsewhere. For example, we have box A and box B in certain positions but now box A and box B positions change.
-   When other agents get to the objectives first like if turtlebot A outputs go to box A but turtlebot B also samples to go to box A but it gets there first. Then, turtlebot A would need to cancel the macro-action and sample another one. (This needs more thought. Some set of macro actions to do stuff but not the best.)
-   Collaboration: turtlebot A goes to small box, but then it sees turtlebot B going into same area. Then both of them can cancel macro-action and sample at the same time a collaborative macro-action like go to big box since the big box can only be pushed with two turtlebots.

---

## Learning Termination Condition

1.  $\beta_{m_i}(h^A_t) = Q(h,m) - V(h)$
2.  $Q(h,m') - Q(h,m)$

---

## Continuous Macro-Action Policy Gradient Theorem

Using Yuchen's Macro-Action Policy Gradient Theorem as a base:

$$\begin{equation}
V^{\Psi}(h) = \int_{\mathcal{M}}\Psi(m|h)Q^{\Psi}(h, m)dm \text{ (How good is h over policy)}
\end{equation}$$

$$\begin{equation}
Q^{\Psi}(h, m) = r^c(h, m) + \int_{h'}P(h'|h, m)V^{\Psi}(h')dh' \text{ (How good is taking macro-action over history)}
\end{equation}$$

where,

$$\begin{equation}
r^c(h, m) = \mathbb{E}_{\tau \sim \beta_m, s|_{t_m}}[\ \sum_{t=t_m}^{t_m+\tau-1} \gamma^t r_t\ ]
\end{equation}$$

$$\begin{align}
P(h'|h, m) &= P(z'|h, m) = \sum_{\tau=1}^{\infty}\gamma^{\tau}P(z', \tau|h, m) \\
&= \sum_{\tau=1}^{\infty}\gamma^{\tau}P(\tau|h, m)P(z'|h, m, \tau)  \\
&= \mathbb{E}_{\tau \sim \beta_m}[\gamma^{\tau}\mathbb{E}_{s|h}[\mathbb{E}_{s'|s,m,\tau}[P(z'|m,s')]]]
\end{align}$$

Next, we follow the proof of the policy gradient theorem (Sutton et al. 2000):

$$\begin{align}
\nabla_{\theta}V^{\Psi_{\theta}}(h)
&=
\nabla_{\theta}\Bigl[\,
  \int_{\mathcal M}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)\,dm
\Bigr]
\\
&=
\int_{\mathcal M}
\Bigl[
  \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)
  \;+\;
  \Psi_{\theta}(m\mid h)\,\nabla_{\theta}Q^{\Psi_{\theta}}(h,m)
\Bigr]
\,dm
\\
&=
\int_{\mathcal M}
\Bigl[
  \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)
  \;+\; 
  \Psi_{\theta}(m\mid h)\,
  \nabla_{\theta}\Bigl(r^c(h,m)
    + \int_{h'}P(h'\!\mid h,m)\,V^{\Psi_{\theta}}(h')\,dh'
  \Bigr)
\Bigr]
\,dm
\\
&=
\int_{\mathcal M}
\Bigl[
  \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)
  +
  \Psi_{\theta}(m\mid h)\,
  \int_{h'}P(h'\!\mid h,m)\,\nabla_{\theta}V^{\Psi_{\theta}}(h')\,dh'
\Bigr]
\,dm
\\
&=
\int_{h\in\mathcal H}\!\!
\sum_{k=0}^\infty
  P\bigl(h_0\rightarrow h,\;k,\;\Psi_{\theta}\bigr)\,
  \underbrace{
    \int_{\mathcal M}
      \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)
    \,dm
  }_{\displaystyle(Value\_Function)}
\,dh
\quad\text{(after unrolling)}
\end{align}$$

Then the gradient will be,

$$\begin{align}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}V^{\Psi_{\theta}}(h_0) \\
&= \int_{h\in\mathcal H}\sum_{k=0}^{\infty}P(h_0\rightarrow h,k,\Psi_{\theta})\int_{\mathcal M}\nabla_{\theta}\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh \\
&= \int_{h\in\mathcal H}\rho^{\Psi_{\theta}}(h)\int_{\mathcal M}\nabla_{\theta}\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh
\end{align}$$

Where $\rho^{\Psi_{\theta}}(h)$ represents the discounted state visitation distribution under policy $\Psi_{\theta}$. Applying the log derivative trick $\nabla_{\theta}\Psi_{\theta}(m|h) = \Psi_{\theta}(m|h)\nabla_{\theta}\log\Psi_{\theta}(m|h)$:

$$\begin{align}
\nabla_{\theta}J(\theta) &= \int_{h\in\mathcal H}\rho^{\Psi_{\theta}}(h)\int_{\mathcal M}\Psi_{\theta}(m|h)\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh \\
&= \mathbb{E}_{h\sim\rho^{\Psi_{\theta}}}\Bigl[\mathbb{E}_{m\sim\Psi_{\theta}(\cdot|h)}\bigl[\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\bigr]\Bigr] \\
&= \mathbb{E}_{h\sim\rho^{\Psi_{\theta}},m\sim\Psi_{\theta}}\bigl[\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\bigr]
\end{align}$$
---
layout: post
title: "Continuous Macro Actions"
tags: [Research, RL]
---

## What is a Macro-Action

## Continuous Macro Actions 
Writing Research paper on extending Macro-Actions to the continuous case. 
## Notation and Function Definitions

-   $Q^{\theta_i}_{\phi_i}(h_i, m_i)$ is the decentralized critic
-   $Q^{\vec{\Psi}}_{\phi}(\vec{h}, \vec{m})$ is the centralized critic
-   $\Psi_{\theta_i}(m_i \mid h_i)$ is the macro-action-based policy and is the individual actor
-   $\Psi_{\theta}(\vec{m} \mid \vec{h})$ is the centralized actor or joint macro-action-based policy
-   $V_{\mathbf{w}_i}^{\Psi_{\theta_i}}(h_i)$ is the local history value function or critic
-   $V_{\mathbf{w}}^{\Psi_{\theta}}(\vec{h})$ is the centralized history value function or critic
-   $V_{\mathbf{w_i}}^{\vec{\Psi}_{\theta_i}}(\vec{h_i})$ is the separate centralized critic used in MAPPO
-   $r_i^c$ is the cumulative-discounted reward of the macro-action taking $\tau_{mi}$ time steps from beginning.
-   $Q_{\phi_i}(h_i, m_i)$ is the individual macro-action-value function
-   $Q_{\phi_i}(\vec{h}_i, \vec{m}_i)$ is the joint macro-action value function
-   $x$ represents the available centralized information
-   $\vec{A}$ advantage calculated using centralized information using GAE
-   $\alpha$ is the positive coefficient for clipping function
-   $\beta$ is the negative coefficient for the clipping function

---

## Parameterized/Continuous Macro-Action

**Reason/Motivation:** It is mainly for scalability. If we use discrete macro-actions, and it is not parameterized, then for any large grid world, we would have a problem because it would be length\*width amount of macro-actions to get to every point on the grid (same way of why we use continuous actions). With continuous macro-actions, we could solve that because we can learn the macro action that gets to where we want without having to store all the discrete macro-actions. This could translate to real-world control problems with robotics. There are many examples out there already on robots learning high-level actions in a hierarchical manner so they are able to collaborate with one another. What we are doing here is formalizing that in a MARL manner as a multi-agent way to do multi-robot collaboration in the real world.

Continuous Macro-Actions can have the underlying policy running discrete or continuous primitive actions. In this paper, we will just define the high-level policy in the continuous space but the lower-level policy will still be discrete. It is also possible to define both the high and low level policies to be continuous. In essence, high level is continuous but the lower level can be discrete or continuous.

Also real world robotics do not use discrete actions as robots can’t teleport to a place. It would need to move in continuous time to get to a certain area. We can form it in a way that looks discrete but the underlying actions are still continuous.

We can learn the termination condition by adding it to the MAPPO. Since the termination condition is learned, we can have macro-actions that can be more finely controlled. Other than the policy being continuous instead of discrete, the equations would remain the same.

#### Continuous Macro-Actions:

We assume that every continuous macro-action
$$ m \;\in\;\mathcal M\;\subset\;\mathbb R^d $$
can be *deterministically* decomposed into a sequence of primitive actions in each agent's primitive action set $\mathcal A_i\subset\mathbb R^k$. Concretely, there exists $(a_0, a_1, \dots, a_{\tau-1})$, where each
$$ a_t \;\in\; A \;=\;\bigtimes_{i\in I} A_i $$
In this case, $\tau$ is the length of the macro-action.

1.  There is a deterministic map
    $$ \phi : A^\tau \;\longrightarrow\;\mathcal M, \qquad m \;=\;\phi\bigl(a_0, a_1, \dots, a_{\tau-1}\bigr). $$
2.  During execution of $m$, at each underlying step $t=0,\dots,\tau-1$, the agent executes a discrete primitve action based on the local primitve observation history $H^A_i$.
3.  The macro-observation $z$ seen at macro-termination can itself be viewed as a function of the primitive observations: $\psi: \Omega^\tau\to\mathcal Z$.

#### Continuous macro-action as a Mac-DecPOMDP:

In the MacDec-POMDP formalization each agent’s continuous macro-action $m_i \in M_i \subset \mathbb R^d$ is viewed as an *option*, i.e. a tuple
$$ m_i \;=\;\bigl\langle I_{m_i},\;\pi_{m_i},\;\beta_{m_i}\bigr\rangle, $$
where:
-   $I_{m_i}\subset H^M_i$ is an *initiation set*, the set of macro-histories $h^i\in H^M_i$ from which this macro-action can begin.
-   $\pi_{m_i}(a\mid h^A_t)$ is a continuous *intra-option policy* over the primitive-action space. $H_i^A \times A_i \rightarrow [0, 1]$. While the macro-action is active, at each primitive time-step $t$ the agent samples
    $$ a_t \;\sim\;\pi_{m_i}(\,\cdot\mid h^i_t)\,,\quad a_t\in A, $$
    transitions $T(s_{t+1}\!\mid s_t,a_t)$, and observes new $o_{t+1}$.
-   $\beta_{m_i}(h^A_t)\in[0,1]$ is a *termination function*. At each $t$ the option terminates with probability $\beta_{m_i}(h^A_t)$, yielding a random duration $\tau_i\sim\mathrm{T}\bigl(h^i_t;\beta^i_{m}\bigr)$. The idea is that it will randomly choose an ending point at some time step $t | h$. Learning policy over m as well as policy over b. Or say that previously it is predefined but now we assume it is not. Write as two seperate functions or as one thing. Answer when do I learn the termination function and when do I learn the other one? What would be the value if i stopped vs continued.

When the option terminates (at primitive step $\tau_i$), there will be a new *macro-observation* $z^i$ via
$$ Z_i\bigl(z^i\mid m^i,\,s_{\tau_i}\bigr) = P\bigl(z^i\mid h^i_{\tau_i},m^i\bigr), $$
and its high-level macro-action observation history $h^i\in H^M_i$.

Under these definitions, we will have the high-level policy
$$ \Psi_i\;(m^i\mid h^i)\,,\quad m^i\in M_i $$
becoming the density over options.

---

## Problems?

-   When objectives suddenly change like the box suddenly no longer exist in the preset position so it needs to cancel the macro-action and choose to go elsewhere. For example, we have box A and box B in certain positions but now box A and box B positions change.
-   When other agents get to the objectives first like if turtlebot A outputs go to box A but turtlebot B also samples to go to box A but it gets there first. Then, turtlebot A would need to cancel the macro-action and sample another one. (This needs more thought. Some set of macro actions to do stuff but not the best.)
-   Collaboration: turtlebot A goes to small box, but then it sees turtlebot B going into same area. Then both of them can cancel macro-action and sample at the same time a collaborative macro-action like go to big box since the big box can only be pushed with two turtlebots.

---

## Learning Termination Condition

1.  $\beta_{m_i}(h^A_t) = Q(h,m) - V(h)$
2.  $Q(h,m') - Q(h,m)$

---

## Continuous Macro-Action Policy Gradient Theorem

Using Yuchen's Macro-Action Policy Gradient Theorem as a base:
$$ V^{\Psi}(h) = \int_{M}\Psi(m|h)Q^{\Psi}(h, m)dm $$
(How good is h over policy)
$$ Q^{\Psi}(h, m) = r^c(h, m) + \int_{h'}P(h'|h, m)V^{\Psi}(h')dh' $$
(How good is taking macro-action over history)
where,
$$ r^c(h, m) = \mathbb{E}_{\tau \sim \beta_m, s|_{t_m}}[\ \sum_{t=t_m}^{t_m+\tau-1} \gamma^t r_t\ ] $$
\begin{align}
P(h'|h, m) &= P(z'|h, m) = \sum_{\tau=1}^{\infty}\gamma^{\tau}P(z', \tau|h, m) \\
&= \sum_{\tau=1}^{\infty}\gamma^{\tau}P(\tau|h, m)P(z'|h, m, \tau) \\
&= \sum_{\tau=1}^{\infty}\gamma^{\tau}P(\tau|h, m)P(z'|h, m, \tau) \\
&= \mathbb{E}_{\tau \sim \beta_m}[\gamma^{\tau}\mathbb{E}_{s|h}[\mathbb{E}_{s'|s,m,\tau}[P(z'|m,s')]]]
\end{align}
Next, we follow the proof of the policy gradient theorem (Sutton et al. 2000):
\begin{align}
\nabla_{\theta}V^{\Psi_{\theta}}(h) &= \nabla_{\theta}\Bigl[\, \int_{\mathcal M}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m)\,dm \Bigr] \\
&= \int_{\mathcal M} \Bigl[ \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m) \;+\; \Psi_{\theta}(m\mid h)\,\nabla_{\theta}Q^{\Psi_{\theta}}(h,m) \Bigr] \,dm \\
&= \int_{\mathcal M} \Bigl[ \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m) \;+\; \Psi_{\theta}(m\mid h)\, \nabla_{\theta}\Bigl(r^c(h,m) + \int_{h'}P(h'\!\mid h,m)\,V^{\Psi_{\theta}}(h')\,dh' \Bigr) \Bigr] \,dm \\
&= \int_{\mathcal M} \Bigl[ \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m) + \Psi_{\theta}(m\mid h)\, \int_{h'}P(h'\!\mid h,m)\,\nabla_{\theta}V^{\Psi_{\theta}}(h')\,dh' \Bigr] \,dm \\
&= \int_{h\in\mathcal H}\!\! \sum_{k=0}^\infty P\bigl(h_0\rightarrow h,\;k,\;\Psi_{\theta}\bigr)\, \underbrace{ \int_{\mathcal M} \nabla_{\theta}\Psi_{\theta}(m\mid h)\,Q^{\Psi_{\theta}}(h,m) \,dm }_{\displaystyle(Value\_Function)} \,dh \quad\text{(after unrolling)}
\end{align}
Then the gradient will be,
\begin{align}
\nabla_{\theta}J(\theta) &= \nabla_{\theta}V^{\Psi_{\theta}}(h_0) \\
&= \int_{h\in\mathcal H}\sum_{k=0}^{\infty}P(h_0\rightarrow h,k,\Psi_{\theta})\int_{\mathcal M}\nabla_{\theta}\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh \\
&= \int_{h\in\mathcal H}\rho^{\Psi_{\theta}}(h)\int_{\mathcal M}\nabla_{\theta}\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh
\end{align}
\begin{align}
\nabla_{\theta}J(\theta) &= \int_{h\in\mathcal H}\rho^{\Psi_{\theta}}(h)\int_{\mathcal M}\Psi_{\theta}(m|h)\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\,dm\,dh \\
&= \mathbb{E}_{h\sim\rho^{\Psi_{\theta}}}\Bigl[\mathbb{E}_{m\sim\Psi_{\theta}(\cdot|h)}\bigl[\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\bigr]\Bigr] \\
&= \mathbb{E}_{h\sim\rho^{\Psi_{\theta}},m\sim\Psi_{\theta}}\bigl[\nabla_{\theta}\log\Psi_{\theta}(m|h)Q^{\Psi_{\theta}}(h,m)\bigr]
\end{align}
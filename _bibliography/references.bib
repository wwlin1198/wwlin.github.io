
@inproceedings{amato2015decentralized,
  title={Decentralized Control of Partially Observable Markov Decision Processes using Belief Space Macro-actions},
  author={Amato, Christopher and Chowdhary, Girish and Geramifard, Alborz and Üre, N. Kemal and Kochenderfer, Mykel J.},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  pages={547--553},
  year={2015}
}

@inproceedings{amato2014scalable,
  title={Scalable Planning and Learning for Multiagent POMDPs},
  author={Amato, Christopher and Oliehoek, Frans A.},
  booktitle={Proceedings of the National Conference on Artificial Intelligence (AAAI)},
  volume={2},
  pages={1995--2002},
  year={2014}
}

@article{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  pages={1057--1063},
  year={2000}
}

@misc{zhao_optimistic_2024,
	title = {Optimistic {Multi}-{Agent} {Policy} {Gradient}},
	url = {http://arxiv.org/abs/2311.01953},
	doi = {10.48550/arXiv.2311.01953},
	abstract = {*Relative overgeneralization* (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. No methods have been proposed for addressing RO in multi-agent policy gradient (MAPG) methods although these methods produce state-of-the-art results. To address this gap, we propose a general, yet simple, framework to enable optimistic updates in MAPG methods that alleviate the RO problem. Our approach involves clipping the advantage to eliminate negative values, thereby facilitating optimistic updates in MAPG. The optimism prevents individual agents from quickly converging to a local optimum. Additionally, we provide a formal analysis to show that the proposed method retains optimality at a fixed point. In extensive evaluations on a diverse set of tasks including the *Multi-agent MuJoCo* and *Overcooked* benchmarks, our method outperforms strong baselines on 13 out of 19 tested tasks and matches the performance on the rest.},
	urldate = {2025-09-16},
	publisher = {arXiv},
	author = {Zhao, Wenshuai and Zhao, Yi and Li, Zhiyuan and Kannala, Juho and Pajarinen, Joni},
	month = may,
	year = {2024},
	note = {arXiv:2311.01953 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{amato_initial_2025,
	title = {An {Initial} {Introduction} to {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2405.06161},
	doi = {10.48550/arXiv.2405.06161},
	abstract = {Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. While numerous approaches have been developed, they can be broadly categorized into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and decentralized training and execution (DTE). CTE methods assume centralization during training and execution (e.g., with fast, free, and perfect communication) and have the most information during execution. CTDE methods are the most common, as they leverage centralized information during training while enabling decentralized execution -- using only information available to that agent during execution. Decentralized training and execution methods make the fewest assumptions and are often simple to implement. This text is an introduction to cooperative MARL -- MARL in which all agents share a single, joint reward. It is meant to explain the setting, basic concepts, and common methods for the CTE, CTDE, and DTE settings. It does not cover all work in cooperative MARL as the area is quite extensive. I have included work that I believe is important for understanding the main concepts in the area and apologize to those that I have omitted. Topics include simple applications of single-agent methods to CTE as well as some more scalable methods that exploit the multi-agent structure, independent Q-learning and policy gradient methods and their extensions, as well as value function factorization methods including the well-known VDN, QMIX, and QPLEX approaches, and centralized critic methods including MADDPG, COMA, and MAPPO. I also discuss common misconceptions, the relationship between different approaches, and some open questions.},
	urldate = {2025-09-11},
	publisher = {arXiv},
	author = {Amato, Christopher},
	month = may,
	year = {2025},
	note = {arXiv:2405.06161 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{wu_learning_2025,
	title = {Learning {Individual} {Intrinsic} {Reward} in {Multi}-{Agent} {Reinforcement} {Learning} via {Incorporating} {Generalized} {Human} {Expertise}},
	url = {http://arxiv.org/abs/2507.18867},
	doi = {10.48550/arXiv.2507.18867},
	abstract = {Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Wu, Xuefei and Yin, Xiao and Zhu, Yuanyang and Chen, Chunlin},
	month = jul,
	year = {2025},
	note = {arXiv:2507.18867 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{fu_revisiting_2022,
	title = {Revisiting {Some} {Common} {Practices} in {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2206.07505},
	doi = {10.48550/arXiv.2206.07505},
	abstract = {Many advances in cooperative multi-agent reinforcement learning (MARL) are based on two common design principles: value decomposition and parameter sharing. A typical MARL algorithm of this fashion decomposes a centralized Q-function into local Q-networks with parameters shared across agents. Such an algorithmic paradigm enables centralized training and decentralized execution (CTDE) and leads to efficient learning in practice. Despite all the advantages, we revisit these two principles and show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, value decomposition, and parameter sharing can be problematic and lead to undesired outcomes. In contrast, policy gradient (PG) methods with individual policies provably converge to an optimal solution in these cases, which partially supports some recent empirical observations that PG can be effective in many MARL testbeds. Inspired by our theoretical analysis, we present practical suggestions on implementing multi-agent PG algorithms for either high rewards or diverse emergent behaviors and empirically validate our findings on a variety of domains, ranging from the simplified matrix and grid-world games to complex benchmarks such as StarCraft Multi-Agent Challenge and Google Research Football. We hope our insights could benefit the community towards developing more general and more powerful MARL algorithms. Check our project website at https://sites.google.com/view/revisiting-marl.},
	urldate = {2025-07-30},
	publisher = {arXiv},
	author = {Fu, Wei and Yu, Chao and Xu, Zelai and Yang, Jiaqi and Wu, Yi},
	month = aug,
	year = {2022},
	note = {arXiv:2206.07505 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{jung_agent-centric_nodate,
	title = {Agent-{Centric} {Actor}-{Critic} for {Asynchronous} {Multi}-{Agent} {Reinforcement} {Learning}},
	abstract = {Multi-Agent Reinforcement Learning (MARL) struggles with coordination in sparse reward environments. Macro-actions —sequences of actions executed as single decisions— facilitate longterm planning but introduce asynchrony, complicating Centralized Training with Decentralized Execution (CTDE). Existing CTDE methods use padding to handle asynchrony, risking misaligned asynchronous experiences and spurious correlations. We propose the Agent-Centric Actor-Critic (ACAC) algorithm to manage asynchrony without padding. ACAC uses agent-centric encoders for independent trajectory processing, with an attention-based aggregation module integrating these histories into a centralized critic for improved temporal abstractions. The proposed structure is trained via a PPO-based algorithm with a modified Generalized Advantage Estimation for asynchronous environments. Experiments show ACAC accelerates convergence and enhances performance over baselines in complex MARL tasks.},
	language = {en},
	author = {Jung, Whiyoung and Hong, Sunghoon and Yoon, Deunsol and Lee, Kanghoon and Lim, Woohyung},
}

@article{albrecht_multi-agent_nodate,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Foundations} and {Modern} {Approaches}},
	language = {en},
	author = {Albrecht, Stefano V and Christianos, Filippos and Schäfer, Lukas},
}

@article{hu_mo-mix_2023,
	title = {{MO}-{MIX}: {Multi}-{Objective} {Multi}-{Agent} {Cooperative} {Decision}-{Making} {With} {Deep} {Reinforcement} {Learning}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {{MO}-{MIX}},
	url = {https://ieeexplore.ieee.org/document/10145811},
	doi = {10.1109/TPAMI.2023.3283537},
	abstract = {Deep reinforcement learning (RL) has been applied extensively to solve complex decision-making problems. In many real-world scenarios, tasks often have several conflicting objectives and may require multiple agents to cooperate, which are the multi-objective multi-agent decision-making problems. However, only few works have been conducted on this intersection. Existing approaches are limited to separate fields and can only handle multi-agent decision-making with a single objective, or multi-objective decision-making with a single agent. In this paper, we propose MO-MIX to solve the multi-objective multi-agent reinforcement learning (MOMARL) problem. Our approach is based on the centralized training with decentralized execution (CTDE) framework. A weight vector representing preference over the objectives is fed into the decentralized agent network as a condition for local action-value function estimation, while a mixing network with parallel architecture is used to estimate the joint action-value function. In addition, an exploration guide approach is applied to improve the uniformity of the final non-dominated solutions. Experiments demonstrate that the proposed method can effectively solve the multi-objective multi-agent cooperative decision-making problem and generate an approximation of the Pareto set. Our approach not only significantly outperforms the baseline method in all four kinds of evaluation metrics, but also requires less computational cost.},
	number = {10},
	urldate = {2025-05-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hu, Tianmeng and Luo, Biao and Yang, Chunhua and Huang, Tingwen},
	month = oct,
	year = {2023},
	keywords = {Approximation algorithms, Behavioral sciences, Decision making, Deep reinforcement learning, Multi-agent systems, Pareto, Reinforcement learning, Task analysis, Training, decision-making, multi-agent, multi-objective},
	pages = {12098--12112},
}

@article{hu_review_2024,
	title = {A review of research on reinforcement learning algorithms for multi-agents},
	volume = {599},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224008397},
	doi = {10.1016/j.neucom.2024.128068},
	abstract = {In recent years, multi-agent reinforcement learning techniques have been widely used and evolved in the field of artificial intelligence. However, traditional reinforcement learning methods have limitations such as long training time, large sample data requirements, and highly delayed rewards. Therefore, this paper systematically and specifically studies the MARL algorithm. Firstly, this paper uses Citespace software to visually analyze the existing literature on multi-agent reinforcement learning and briefly indicates the research hotspots and key research directions in this field. Secondly, the applications of traditional reinforcement learning algorithms under two task objects, namely single-agent and multi-agent systems, are described in detail. Then, the paper highlights the diverse applications, challenges, and corresponding solutions of MARL algorithmic techniques in the field of MAS. Finally, the paper points out future research directions based on the existing limitations of the algorithm. Through this paper, readers will gain a systematic and in-depth understanding of MARL algorithms and how they can be utilized to better address the various challenges posed by MAS.},
	urldate = {2025-05-19},
	journal = {Neurocomputing},
	author = {Hu, Kai and Li, Mingyang and Song, Zhiqiang and Xu, Keer and Xia, Qingfeng and Sun, Ning and Zhou, Peng and Xia, Min},
	month = sep,
	year = {2024},
	keywords = {Agent, Multi-agent reinforcement learning, Multi-agent systems, Reinforcement learning},
	pages = {128068},
}

@article{hu_review_2024-1,
	title = {A review of research on reinforcement learning algorithms for multi-agents},
	volume = {599},
	issn = {0925-2312},
	url = {https://doi.org/10.1016/j.neucom.2024.128068},
	doi = {10.1016/j.neucom.2024.128068},
	number = {C},
	urldate = {2025-05-19},
	journal = {Neurocomput.},
	author = {Hu, Kai and Li, Mingyang and Song, Zhiqiang and Xu, Keer and Xia, Qingfeng and Sun, Ning and Zhou, Peng and Xia, Min},
	month = sep,
	year = {2024},
}

@article{hu_review_2024-2,
	title = {A review of research on reinforcement learning algorithms for multi-agents},
	volume = {599},
	issn = {0925-2312},
	url = {https://doi.org/10.1016/j.neucom.2024.128068},
	doi = {10.1016/j.neucom.2024.128068},
	number = {C},
	urldate = {2025-05-19},
	journal = {Neurocomput.},
	author = {Hu, Kai and Li, Mingyang and Song, Zhiqiang and Xu, Keer and Xia, Qingfeng and Sun, Ning and Zhou, Peng and Xia, Min},
	month = sep,
	year = {2024},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options deﬁned over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. ! 1999 Published by Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-04-21},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	month = aug,
	year = {1999},
	pages = {181--211},
}

@misc{yu_asynchronous_2023,
	title = {Asynchronous {Multi}-{Agent} {Reinforcement} {Learning} for {Efficient} {Real}-{Time} {Multi}-{Robot} {Cooperative} {Exploration}},
	url = {http://arxiv.org/abs/2301.03398},
	doi = {10.48550/arXiv.2301.03398},
	abstract = {We consider the problem of cooperative exploration where multiple robots need to cooperatively explore an unknown region as fast as possible. Multi-agent reinforcement learning (MARL) has recently become a trending paradigm for solving this challenge. However, existing MARL-based methods adopt action-making steps as the metric for exploration efficiency by assuming all the agents are acting in a fully synchronous manner: i.e., every single agent produces an action simultaneously and every single action is executed instantaneously at each time step. Despite its mathematical simplicity, such a synchronous MARL formulation can be problematic for real-world robotic applications. It can be typical that different robots may take slightly different wall-clock times to accomplish an atomic action or even periodically get lost due to hardware issues. Simply waiting for every robot being ready for the next action can be particularly time-inefficient. Therefore, we propose an asynchronous MARL solution, Asynchronous Coordination Explorer (ACE), to tackle this real-world challenge. We first extend a classical MARL algorithm, multi-agent PPO (MAPPO), to the asynchronous setting and additionally apply action-delay randomization to enforce the learned policy to generalize better to varying action delays in the real world. Moreover, each navigation agent is represented as a team-size-invariant CNN-based policy, which greatly benefits real-robot deployment by handling possible robot lost and allows bandwidth-efficient intra-agent communication through low-dimensional CNN features. We first validate our approach in a grid-based scenario. Both simulation and real-robot results show that ACE reduces over 10\% actual exploration time compared with classical approaches. We also apply our framework to a high-fidelity visual-based environment, Habitat, achieving 28\% improvement in exploration efficiency.},
	urldate = {2025-04-10},
	publisher = {arXiv},
	author = {Yu, Chao and Yang, Xinyi and Gao, Jiaxuan and Chen, Jiayu and Li, Yunfei and Liu, Jijia and Xiang, Yunfei and Huang, Ruixin and Yang, Huazhong and Wu, Yi and Wang, Yu},
	month = apr,
	year = {2023},
	note = {arXiv:2301.03398 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{eysenbach_diversity_2018,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	shorttitle = {Diversity is {All} {You} {Need}},
	url = {http://arxiv.org/abs/1802.06070},
	doi = {10.48550/arXiv.1802.06070},
	abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	month = oct,
	year = {2018},
	note = {arXiv:1802.06070 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{peng_facmac_2021,
	title = {{FACMAC}: {Factored} {Multi}-{Agent} {Centralised} {Policy} {Gradients}},
	shorttitle = {{FACMAC}},
	url = {http://arxiv.org/abs/2003.06709},
	doi = {10.48550/arXiv.2003.06709},
	abstract = {We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Like MADDPG, a popular multi-agent actor-critic method, our approach uses deep deterministic policy gradients to learn policies. However, FACMAC learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX, a popular multi-agent Q-learning algorithm. However, unlike QMIX, there are no inherent constraints on factoring the critic. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent's action space separately as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of a centralised critic. We evaluate FACMAC on variants of the multi-agent particle environments, a novel multi-agent MuJoCo benchmark, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC's superior performance over MADDPG and other baselines on all three domains.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Peng, Bei and Rashid, Tabish and Witt, Christian A. Schroeder de and Kamienny, Pierre-Alexandre and Torr, Philip H. S. and Böhmer, Wendelin and Whiteson, Shimon},
	month = may,
	year = {2021},
	note = {arXiv:2003.06709 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{klissarov_flexible_2021,
	title = {Flexible {Option} {Learning}},
	url = {https://www.semanticscholar.org/paper/Flexible-Option-Learning-Klissarov-Precup/7372b0a5f456eba1788e9bd5776c1214f9933bc3},
	abstract = {Temporal abstraction in reinforcement learning (RL), offers the promise of improving generalization and knowledge transfer in complex environments, by propagating information more efficiently over time. Although option learning was initially formulated in a way that allows updating many options simultaneously, using off-policy, intra-option learning (Sutton, Precup\&Singh, 1999), many of the recent hierarchical reinforcement learning approaches only update a single option at a time: the option currently executing. We revisit and extend intra-option learning in the context of deep reinforcement learning, in order to enable updating all options consistent with current primitive action choices, without introducing any additional estimates. Our method can therefore be naturally adopted in most hierarchical RL frameworks. When we combine our approach with the option-critic algorithm for option discovery, we obtain significant improvements in performance and data-efficiency across a wide variety of domains.},
	urldate = {2025-04-01},
	author = {Klissarov, Martin and Precup, Doina},
	month = dec,
	year = {2021},
}

@misc{chunduru_attention_2022,
	title = {Attention {Option}-{Critic}},
	url = {http://arxiv.org/abs/2201.02628},
	doi = {10.48550/arXiv.2201.02628},
	abstract = {Temporal abstraction in reinforcement learning is the ability of an agent to learn and use high-level behaviors, called options. The option-critic architecture provides a gradient-based end-to-end learning method to construct options. We propose an attention-based extension to this framework, which enables the agent to learn to focus different options on different aspects of the observation space. We show that this leads to behaviorally diverse options which are also capable of state abstraction, and prevents the degeneracy problems of option domination and frequent option switching that occur in option-critic, while achieving a similar sample complexity. We also demonstrate the more efficient, interpretable, and reusable nature of the learned options in comparison with option-critic, through different transfer learning tasks. Experimental results in a relatively simple four-rooms environment and the more complex ALE (Arcade Learning Environment) showcase the efficacy of our approach.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Chunduru, Raviteja and Precup, Doina},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02628 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{chakravorty_option-critic_2020,
	title = {Option-{Critic} in {Cooperative} {Multi}-agent {Systems}},
	url = {http://arxiv.org/abs/1911.12825},
	doi = {10.48550/arXiv.1911.12825},
	abstract = {In this paper, we investigate learning temporal abstractions in cooperative multi-agent systems, using the options framework (Sutton et al, 1999). First, we address the planning problem for the decentralized POMDP represented by the multi-agent system, by introducing a {\textbackslash}emph\{common information approach\}. We use the notion of {\textbackslash}emph\{common beliefs\} and broadcasting to solve an equivalent centralized POMDP problem. Then, we propose the Distributed Option Critic (DOC) algorithm, which uses centralized option evaluation and decentralized intra-option improvement. We theoretically analyze the asymptotic convergence of DOC and build a new multi-agent environment to demonstrate its validity. Our experiments empirically show that DOC performs competitively against baselines and scales with the number of agents.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Chakravorty, Jhelum and Ward, Nadeem and Roy, Julien and Chevalier-Boisvert, Maxime and Basu, Sumana and Lupu, Andrei and Precup, Doina},
	month = mar,
	year = {2020},
	note = {arXiv:1911.12825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@misc{li_soac_2020,
	title = {{SOAC}: {The} {Soft} {Option} {Actor}-{Critic} {Architecture}},
	shorttitle = {{SOAC}},
	url = {http://arxiv.org/abs/2006.14363},
	doi = {10.48550/arXiv.2006.14363},
	abstract = {The option framework has shown great promise by automatically extracting temporally-extended sub-tasks from a long-horizon task. Methods have been proposed for concurrently learning low-level intra-option policies and high-level option selection policy. However, existing methods typically suffer from two major challenges: ineffective exploration and unstable updates. In this paper, we present a novel and stable off-policy approach that builds on the maximum entropy model to address these challenges. Our approach introduces an information-theoretical intrinsic reward for encouraging the identification of diverse and effective options. Meanwhile, we utilize a probability inference model to simplify the optimization problem as fitting optimal trajectories. Experimental results demonstrate that our approach significantly outperforms prior on-policy and off-policy methods in a range of Mujoco benchmark tasks while still providing benefits for transfer learning. In these tasks, our approach learns a diverse set of options, each of whose state-action space has strong coherence.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Li, Chenghao and Ma, Xiaoteng and Zhang, Chongjie and Yang, Jun and Xia, Li and Zhao, Qianchuan},
	month = jun,
	year = {2020},
	note = {arXiv:2006.14363 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bacon_option-critic_2016,
	title = {The {Option}-{Critic} {Architecture}},
	url = {http://arxiv.org/abs/1609.05140},
	doi = {10.48550/arXiv.1609.05140},
	abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	month = dec,
	year = {2016},
	note = {arXiv:1609.05140 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{reddy_latent_nodate,
	title = {Latent {Skill} {Embedding} for {Personalized} {Lesson} {Sequence} {Recommendation}},
	abstract = {Students in online courses generate large amounts of data that can be used to personalize the learning process and improve quality of education. In this paper, we present the Latent Skill Embedding (LSE), a probabilistic model of students and educational content that can be used to recommend personalized sequences of lessons with the goal of helping students prepare for speciﬁc assessments. Akin to collaborative ﬁltering for recommender systems, the algorithm does not require students or content to be described by features, but it learns a representation using access traces. We formulate this problem as a regularized maximum-likelihood embedding of students, lessons, and assessments from historical student-content interactions. An empirical evaluation on large-scale data from Knewton, an adaptive learning technology company, shows that this approach predicts assessment results competitively with benchmark models and is able to discriminate between lesson sequences that lead to mastery and failure.},
	language = {en},
	author = {Reddy, Siddharth and Labutov, Igor and Joachims, Thorsten},
}

@misc{noauthor_httpsarxivorgpdf171200004_nodate,
	title = {https://arxiv.org/pdf/1712.00004},
	url = {https://arxiv.org/pdf/1712.00004},
	urldate = {2025-04-01},
}

@misc{klissarov_learnings_2017,
	title = {Learnings {Options} {End}-to-{End} for {Continuous} {Action} {Tasks}},
	url = {http://arxiv.org/abs/1712.00004},
	doi = {10.48550/arXiv.1712.00004},
	abstract = {We present new results on learning temporally extended actions for continuous tasks, using the options framework (Sutton et al. [1999b], Precup [2000]). In order to achieve this goal we work with the option-critic architecture (Bacon et al. [2017]) using a deliberation cost and train it with proximal policy optimization (Schulman et al. [2017]) instead of vanilla policy gradient. Results on Mujoco domains are promising, but lead to interesting questions about when a given option should be used, an issue directly connected to the use of initiation sets.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Klissarov, Martin and Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	month = nov,
	year = {2017},
	note = {arXiv:1712.00004 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{omidshafiei_scalable_2017,
	title = {Scalable accelerated decentralized multi-robot policy search in continuous observation spaces},
	url = {https://ieeexplore.ieee.org/document/7989106},
	doi = {10.1109/ICRA.2017.7989106},
	abstract = {This paper presents the first ever approach for solving continuous-observation Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) and their semi-Markovian counterparts, Dec-POSMDPs. This contribution is especially important in robotics, where a vast number of sensors provide continuous observation data. A continuous-observation policy representation is introduced using Stochastic Kernel-based Finite State Automata (SK-FSAs). An SK-FSA search algorithm titled Entropy-based Policy Search using Continuous Kernel Observations (EPSCKO) is introduced and applied to the first ever continuous-observation Dec-POMDP/Dec-POSMDP domain, where it significantly outperforms state-of-the-art discrete approaches. This methodology is equally applicable to Dec-POMDPs and Dec-POSMDPs, though the empirical analysis presented focuses on Dec-POSMDPs due to their higher scalability. To improve convergence, an entropy injection policy search acceleration approach for both continuous and discrete observation cases is also developed and shown to improve convergence rates without degrading policy quality.},
	urldate = {2025-03-29},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Omidshafiei, Shayegan and Amato, Christopher and Liu, Miao and Everett, Michael and How, Jonathan P. and Vian, John},
	month = may,
	year = {2017},
	keywords = {Acceleration, Convergence, Decision making, Entropy, Robot sensing systems, Scalability},
	pages = {863--870},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shao_deepseekmath_2024,
	title = {{DeepSeekMath}: {Pushing} the {Limits} of {Mathematical} {Reasoning} in {Open} {Language} {Models}},
	shorttitle = {{DeepSeekMath}},
	url = {http://arxiv.org/abs/2402.03300},
	doi = {10.48550/arXiv.2402.03300},
	abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
	month = apr,
	year = {2024},
	note = {arXiv:2402.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{dibangoye_optimally_2016,
	title = {Optimally {Solving} {Dec}-{POMDPs} as {Continuous}-{State} {MDPs}},
	volume = {55},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10986},
	doi = {10.1613/jair.4623},
	abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally (NEXP-Complete). As a new way of solving these problems, we introduce the idea of transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function. This approach makes use of the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP methods to be used for the first time. To provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent domains, without losing the ability to converge to an optimal solution.  In particular, we introduce a feature-based heuristic search value iteration (FB-HSVI) algorithm that relies on feature-based compact representations, point-based updates and efficient action selection.  A theoretical analysis demonstrates that FB-HSVI terminates in finite time with an optimal solution. We include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art.},
	language = {en},
	urldate = {2025-03-28},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dibangoye, Jilles Steeve and Amato, Christopher and Buffet, Olivier and Charpillet, François},
	month = feb,
	year = {2016},
	pages = {443--497},
}

@article{zhang_heterogeneous_2024,
	title = {Heterogeneous {Multi}-{Robot} {Cooperation} {With} {Asynchronous} {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {9},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/10301527},
	doi = {10.1109/LRA.2023.3328448},
	abstract = {Multi-robot systems (MRSs) are becoming increasingly important in various domains. However, effective communication and coordination among multiple robots remain significant challenges. In this letter, we introduce a novel architecture for multi-robot decision-making and control based on multi-agent reinforcement learning (MARL). Our architecture can accommodate heterogeneous robots operating asynchronously in different scenarios. We propose an improved practical Q-value mixing network (Qrainbow), which builds on value-decomposition networks and applies the multi-head attention mixer of Qatten and effective components from Rainbow, such as double network, dueling network, and prioritized experience replay. To migrate the algorithm to MRS, we fuse macro-action into Qrainbow and make a slight change to the process of calculating the loss function, enabling Qrainbow to work in asynchronous scenarios. We evaluate our architecture in both the benchmark environment for MARL and a multi-robot environment with varying layouts. In terms of convergence speed and final result, Qrainbow outperforms other state-of-the-art MARL algorithms. Additionally, our architecture achieves superior performance in reducing time costs and avoiding collisions between robots in homogeneous and heterogeneous multi-robot cooperation tasks.},
	number = {1},
	urldate = {2025-03-27},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zhang, Han and Zhang, Xiaohui and Feng, Zhao and Xiao, Xiaohui},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Collision avoidance, Decision making, Heterogeneous networks, Multi-robot systems, Q-learning, Reinforcement learning, Robot kinematics, Task analysis, Training, asynchronous execution, heterogeneous robots, reinforcement learning},
	pages = {159--166},
}

@misc{kannan_smart-llm_2024,
	title = {{SMART}-{LLM}: {Smart} {Multi}-{Agent} {Robot} {Task} {Planning} using {Large} {Language} {Models}},
	shorttitle = {{SMART}-{LLM}},
	url = {http://arxiv.org/abs/2309.10062},
	doi = {10.48550/arXiv.2309.10062},
	abstract = {In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.},
	urldate = {2025-03-27},
	publisher = {arXiv},
	author = {Kannan, Shyam Sundar and Venkatesh, Vishnunandan L. N. and Min, Byung-Cheol},
	month = mar,
	year = {2024},
	note = {arXiv:2309.10062 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{sun_llm-based_2024,
	title = {{LLM}-based {Multi}-{Agent} {Reinforcement} {Learning}: {Current} and {Future} {Directions}},
	shorttitle = {{LLM}-based {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2405.11106},
	doi = {10.48550/arXiv.2405.11106},
	abstract = {In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.},
	urldate = {2025-03-27},
	publisher = {arXiv},
	author = {Sun, Chuanneng and Huang, Songjun and Pompili, Dario},
	month = may,
	year = {2024},
	note = {arXiv:2405.11106 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@article{kim_universal_2022,
	title = {Universal adaptability: {Target}-independent inference that competes with propensity scoring},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Universal adaptability},
	url = {https://pnas.org/doi/full/10.1073/pnas.2108097119},
	doi = {10.1073/pnas.2108097119},
	abstract = {Significance
            We revisit the problem of ensuring statistically valid inferences across diverse target populations from a single source of training data. Our approach builds a surprising technical connection between the inference problem and a technique developed for algorithmic fairness, called “multicalibration.” We derive a correspondence between the fairness goal, to protect subpopulations from miscalibrated predictions, and the statistical goal, to ensure unbiased estimates on target populations. We derive a single-source estimator that provides inferences in any downstream target population, whose performance is comparable to the popular target-specific approach of propensity score reweighting. Our approach can extend the benefits of evidence-based decision-making to communities that do not have the resources to collect high-quality data on their own.
          , 
            The gold-standard approaches for gleaning statistically valid conclusions from data involve random sampling from the population. Collecting properly randomized data, however, can be challenging, so modern statistical methods, including propensity score reweighting, aim to enable valid inferences when random sampling is not feasible. We put forth an approach for making inferences based on available data from a source population that may differ in composition in unknown ways from an eventual target population. Whereas propensity scoring requires a separate estimation procedure for each different target population, we show how to build a single estimator, based on source data alone, that allows for efficient and accurate estimates on any downstream target data. We demonstrate, theoretically and empirically, that our target-independent approach to inference, which we dub “universal adaptability,” is competitive with target-specific approaches that rely on propensity scoring. Our approach builds on a surprising connection between the problem of inferences in unspecified target populations and the multicalibration problem, studied in the burgeoning field of algorithmic fairness. We show how the multicalibration framework can be employed to yield valid inferences from a single source population across a diverse set of target populations.},
	language = {en},
	number = {4},
	urldate = {2025-03-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kim, Michael P. and Kern, Christoph and Goldwasser, Shafi and Kreuter, Frauke and Reingold, Omer},
	month = jan,
	year = {2022},
	pages = {e2108097119},
}

@inproceedings{ostrowski_mixed-method_2022,
	title = {Mixed-{Method} {Long}-{Term} {Robot} {Usage}: {Older} {Adults}' {Lived} {Experience} of {Social} {Robots}},
	shorttitle = {Mixed-{Method} {Long}-{Term} {Robot} {Usage}},
	url = {https://ieeexplore.ieee.org/abstract/document/9889488},
	doi = {10.1109/HRI53351.2022.9889488},
	abstract = {In the past two decades, human-robot interaction (HRI) researchers have increasingly deployed autonomous and reliable robots long-term in various social contexts including the home. Our work provides a mixed-method approach for analyzing older adults' long-term robot usage data patterns combining quantitative data of robot usage logs with qualitative descriptions from participants' own experience. Overall, this provides a fuller picture to how older adults use and experience social robots in their homes. Our work involves a robot hosting period for at least a month (up to 12 months) in older adults' homes with an experience debrief session held a month into the robot hosting time period. We propose reflections on the novelty effect with respect to older adults' usage data and highlight feelings of guilt, the robot's proactivity and movement, meeting (or not meeting) user expectations, and the robot's persona as key aspects of the hosting experience that promoted usage or non-usage. Finally, we provide design guidelines for structuring future mixed-method long-term robot usage studies being mindful of ethical considerations in this space.},
	urldate = {2024-11-07},
	booktitle = {2022 17th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Ostrowski, Anastasia K. and Breazeal, Cynthia and Park, Hae Won},
	month = mar,
	year = {2022},
	keywords = {Design methodology, Ethics, Older adults, Privacy, Reflection, Reliability, Social robots, co-design, long-term usage, mixed method, older adults, social robots},
	pages = {33--42},
}

@misc{kedia_interact_2024,
	title = {{InteRACT}: {Transformer} {Models} for {Human} {Intent} {Prediction} {Conditioned} on {Robot} {Actions}},
	shorttitle = {{InteRACT}},
	url = {http://arxiv.org/abs/2311.12943},
	doi = {10.48550/arXiv.2311.12943},
	abstract = {In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data, which we open-source.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Kedia, Kushal and Bhardwaj, Atiksh and Dan, Prithwish and Choudhury, Sanjiban},
	month = jun,
	year = {2024},
	note = {arXiv:2311.12943},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{rathbun_sleepernets_2024,
	title = {{SleeperNets}: {Universal} {Backdoor} {Poisoning} {Attacks} {Against} {Reinforcement} {Learning} {Agents}},
	shorttitle = {{SleeperNets}},
	url = {http://arxiv.org/abs/2405.20539},
	doi = {10.48550/arXiv.2405.20539},
	abstract = {Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop ``SleeperNets'' as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.},
	urldate = {2024-10-16},
	author = {Rathbun, Ethan and Amato, Christopher and Oprea, Alina},
	month = may,
	year = {2024},
	note = {arXiv:2405.20539},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{palaniappan_efficient_nodate,
	title = {Efficient {Cooperative} {Inverse} {Reinforcement} {Learning}},
	language = {en},
	author = {Palaniappan, Malayandi and Malik, Dhruv and Hadfield-Menell, Dylan and Dragan, Anca and Russell, Stuart},
}

@article{neumeyer_general-sum_2021,
	title = {General-{Sum} {Multi}-{Agent} {Continuous} {Inverse} {Optimal} {Control}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/9357891?casa_token=vMaKHTxgVcsAAAAA:nOw0MpsfOnFaFbWQxXTmRocyXwGDKXju0dlcEkuBsOJihLQW3h03-cVnPMt3nRUV9PVoTnJr},
	doi = {10.1109/LRA.2021.3060411},
	abstract = {Modeling possible future outcomes of robot-human interactions is of importance in the intelligent vehicle and mobile robotics domains. Knowing the reward function that explains the observed behavior of a human agent is advantageous for modeling the behavior with Markov Decision Processes (MDPs). However, learning the rewards that determine the observed actions from data is complicated by interactions. We present a novel inverse reinforcement learning (IRL) algorithm that can infer the reward function in multi-agent interactive scenarios. In particular, the agents may act boundedly rational (i.e., sub-optimal), a characteristic that is typical for human decision making. Additionally, every agent optimizes its own reward function which makes it possible to address non-cooperative setups. In contrast to other methods, the algorithm does not rely on reinforcement learning during inference of the parameters of the reward function. We demonstrate that our proposed method accurately infers the ground truth reward function in two-agent interactive experiments.1},
	number = {2},
	urldate = {2024-10-11},
	journal = {IEEE Robotics and Automation Letters},
	author = {Neumeyer, Christian and Oliehoek, Frans A. and Gavrila, Dariu M.},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Approximation algorithms, Games, Heuristic algorithms, Inference algorithms, Inverse reinforcement learning, Mathematical model, Reinforcement learning, Trajectory, learning from demonstration, reinforcement learning},
	pages = {3429--3436},
}

@inproceedings{reddy_inverse_2012,
	title = {Inverse reinforcement learning for decentralized non-cooperative multiagent systems},
	url = {https://ieeexplore.ieee.org/abstract/document/6378020},
	doi = {10.1109/ICSMC.2012.6378020},
	abstract = {The objective of inverse reinforcement learning (IRL) is to learn an agent's reward function based on either the agent's policies or the observations of the policy. In this paper we address the issue of using inverse reinforcement learning to learn the reward function in a multi agent setting, where the agents can either cooperate or be strictly non-cooperative. The case of cooperataing agents is a subcase of the non-cooperative setting, where the agents collectively try to maximize a common reward function, instead of maximizing their individual reward functions. Here we present an IRL algorithm that considers the case where the policies of the agents are known. We use the framework that was described by Ng and Russell [2001] and extend it for a Multiagent setting. We assume that the agents are rational and follow an optimal policy in the sense of the Nash Equilibrium. These assumptions are very common in Multiagent systems. We show that in the case of known policies we can reduce the Multiagent problem to a distributed solution where the reward function for each agent can be solved independently using a very similar formulation as for the single agent case.},
	urldate = {2024-10-11},
	booktitle = {2012 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Reddy, Tummalapalli Sudhamsh and Gopikrishna, Vamsikrishna and Zaruba, Gergely and Huber, Manfred},
	month = oct,
	year = {2012},
	note = {ISSN: 1062-922X},
	keywords = {Game Theory, Games, General-Sum Stochastic Games, Inverse Reinfocement Learning, Joints, Markov processes, Multiagent Systems, Multiagent systems, Nash equilibrium, Trajectory},
	pages = {1930--1935},
}

@inproceedings{ng_algorithms_2000,
	address = {San Francisco, CA, USA},
	series = {{ICML} '00},
	title = {Algorithms for {Inverse} {Reinforcement} {Learning}},
	isbn = {978-1-55860-707-1},
	urldate = {2024-10-11},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ng, Andrew Y. and Russell, Stuart J.},
	month = jun,
	year = {2000},
	pages = {663--670},
}

@inproceedings{sengadu_suresh_dec-airl_2023,
	address = {Richland, SC},
	series = {{AAMAS} '23},
	title = {Dec-{AIRL}: {Decentralized} {Adversarial} {IRL} for {Human}-{Robot} {Teaming}},
	isbn = {978-1-4503-9432-1},
	shorttitle = {Dec-{AIRL}},
	abstract = {We present a new method for inverse reinforcement learning (IRL) that allows an agent to learn from expert demonstrations and then spontaneously collaborate with a human on the same task. We generalize adversarial IRL (AIRL) to work in a decentralized setting using a decentralized Markov decision process (Dec-MDP) as the underlying model. We posit that a Dec-MDP is a better-suited model for pragmatic multi-agent IRL compared to the multi-agent Markov decision process (MMDP) or the Markov game, which have been utilized thus far. This is because the latter models require an agent to know the global state of the environment, which is impractical in the real world as it may include agent-specific attributes (e.g. joint angles) that may not be directly observable by the other agents. We test our method on two domains: a formative simulated patient assistance scenario and a summative real-world use-inspired domain of sorting onions on a line conveyor. Our method (Dec-AIRL) significantly improves on the previous techniques in both domains. These results indicate that a decentralized multi-agent IRL formalism promotes effective teaming in human-robot collaborative tasks.},
	urldate = {2024-10-11},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Sengadu Suresh, Prasanth and Gui, Yikang and Doshi, Prashant},
	month = may,
	year = {2023},
	pages = {1116--1124},
}

@inproceedings{bogert_multi-robot_2014,
	address = {Richland, SC},
	series = {{AAMAS} '14},
	title = {Multi-robot inverse reinforcement learning under occlusion with interactions},
	isbn = {978-1-4503-2738-1},
	abstract = {We consider the problem of learning the behavior of multiple mobile robots executing fixed trajectories in a common space and possibly interacting with each other in their execution. The mobile robots are observed by a subject robot from a vantage point from which it can observe a portion of their trajectories only. This problem exhibits wide-ranging applications and the specific application we consider here is that of the subject robot who desires to penetrate a simple perimeter patrol by two interacting robots and reach a goal location. Our approach extends single-agent inverse reinforcement learning (IRL) to a multi-robot setting and partial observability, and models the interaction between the mobile robots as equilibrium behavior. IRL provides weights over the features of the robots' reward functions, thereby allowing us to learn their preferences. Subsequently, we derive a Markov decision process based policy for each other robot. We extend a predominant IRL technique and empirically evaluate its performance in our application setting. We show that our approach in the application setting results in significant improvement in the subject's ability to predict the patroller positions at different points in time with a corresponding increase in its successful penetration rate.},
	urldate = {2024-10-11},
	booktitle = {Proceedings of the 2014 international conference on {Autonomous} agents and multi-agent systems},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Bogert, Kenneth and Doshi, Prashant},
	month = may,
	year = {2014},
	pages = {173--180},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ho_generative_2016,
	title = {Generative {Adversarial} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1606.03476},
	doi = {10.48550/arXiv.1606.03476},
	abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Ho, Jonathan and Ermon, Stefano},
	month = jun,
	year = {2016},
	note = {arXiv:1606.03476},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{di_cosmo_parmap_2012,
	title = {The {Parmap} library},
	url = {https://rdicosmo.github.io/parmap/},
	publisher = {Inria and University of Paris and University of Pisa},
	author = {Di Cosmo, Roberto and Danelutto, Marco},
	year = {2012},
}

@misc{the_cgal_project_computational_1996,
	title = {The {Computational} {Geometry} {Algorithms} {Library}},
	url = {https://cgal.org/},
	author = {{The CGAL Project} and {CGAL Editorial Board}},
	year = {1996},
}

@misc{delebecque_scilab_1994,
	title = {Scilab},
	url = {https://www.scilab.org/},
	abstract = {Software for Numerical Computation freely distributed.},
	publisher = {Inria},
	author = {Delebecque, François and Gomez, Claude and Goursat, Maurice and Nikoukhah, Ramine and Steer, Serge and Chancelier, Jean-Philippe},
	year = {1994},
}

@article{anderson_babyj_2003,
	title = {{BabyJ}: from {Object} {Based} to {Class} {Based} {Programming} via {Types}},
	volume = {82},
	number = {7},
	journal = {WOOD},
	author = {Anderson, Christopher and Drossopoulou, Sophia},
	year = {2003},
	pages = {53--81},
}

@article{choi_inverse_2011,
	title = {Inverse {Reinforcement} {Learning} in {Partially} {Observable} {Environments}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/choi11a.html},
	abstract = {Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead.  The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.},
	number = {21},
	urldate = {2024-10-10},
	journal = {Journal of Machine Learning Research},
	author = {Choi, Jaedeug and Kim, Kee-Eung},
	year = {2011},
	pages = {691--730},
}

@article{zhang_asynchronous_2019,
	title = {{ASYNCHRONOUS} {MULTI}-{AGENT} {GENERATIVE} {ADVERSARIAL} {IMITATION} {LEARNING}},
	url = {https://openreview.net/forum?id=Syx33erYwH},
	abstract = {Imitation learning aims to inversely learn a policy from expert demonstrations, which has been extensively studied in the literature for both single-agent setting with Markov decision process (MDP) model, and multi-agent setting with Markov game (MG) model. However, existing approaches for general multi-agent Markov games are not applicable to multi-agent extensive Markov games, where agents make asynchronous decisions following a certain order, rather than simultaneous decisions. We propose a novel framework for asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) under general extensive Markov game settings, and the learned expert policies are proven to guarantee subgame perfect equilibrium (SPE), a more general and stronger equilibrium than Nash equilibrium (NE). The experiment results demonstrate that compared to state-of-the-art baselines, our AMAGAIL model can better infer the policy of each expert agent using their demonstration data collected from asynchronous decision-making scenarios (i.e., extensive Markov games).},
	language = {en},
	urldate = {2024-10-09},
	author = {Zhang, Xin and Huang, Weixiao and Liao, Renjie and Li, Yanhua},
	month = sep,
	year = {2019},
}

@article{unhelkar_human-aware_2018,
	title = {Human-{Aware} {Robotic} {Assistant} for {Collaborative} {Assembly}: {Integrating} {Human} {Motion} {Prediction} {With} {Planning} in {Time}},
	volume = {3},
	issn = {2377-3766},
	shorttitle = {Human-{Aware} {Robotic} {Assistant} for {Collaborative} {Assembly}},
	url = {https://ieeexplore.ieee.org/abstract/document/8307470?casa_token=Qr9vpon4eesAAAAA:ioiVGBXg2XPSXJi7Hr7lEUPYrgJbdG9PFgdcKuEBzI6G-GrH_wgpIoZ8BkIs0Wni1OEckxwSTgM},
	doi = {10.1109/LRA.2018.2812906},
	abstract = {Introducing mobile robots into the collaborative assembly process poses unique challenges for ensuring efficient and safe human-robot interaction. Current human-robot work cells require the robot to cease operating completely whenever a human enters a shared region of the given cell, and the robots do not explicitly model or adapt to the behavior of the human. In this work, we present a human-aware robotic system with single-axis mobility that incorporates both predictions of human motion and planning in time to execute efficient and safe motions during automotive final assembly. We evaluate our system in simulation against three alternative methods, including a baseline approach emulating the behavior of standard safety systems in factories today. We also assess the system within a factory test environment. Through both live demonstration and results from simulated experiments, we show that our approach produces statistically significant improvements in quantitative measures of safety and fluency of interaction.},
	number = {3},
	urldate = {2024-10-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Unhelkar, Vaibhav V. and Lasota, Przemyslaw A. and Tyroller, Quirin and Buhai, Rares-Darius and Marceau, Laurie and Deml, Barbara and Shah, Julie A.},
	month = jul,
	year = {2018},
	keywords = {Collaboration, Engines, Physical human-robot interaction, Planning, Production facilities, Robots, Safety, Task analysis, assembly, collaborative robots},
	pages = {2394--2401},
}

@misc{garg_iq-learn_2021,
	title = {{IQ}-{Learn}: {Inverse} soft-{Q} {Learning} for {Imitation}},
	shorttitle = {{IQ}-{Learn}},
	url = {https://arxiv.org/abs/2106.12142v4},
	abstract = {In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.},
	language = {en},
	urldate = {2024-10-07},
	journal = {arXiv.org},
	author = {Garg, Divyansh and Chakraborty, Shuvam and Cundy, Chris and Song, Jiaming and Geist, Matthieu and Ermon, Stefano},
	month = jun,
	year = {2021},
}

@misc{song_multi-agent_2018,
	title = {Multi-{Agent} {Generative} {Adversarial} {Imitation} {Learning}},
	url = {https://arxiv.org/abs/1807.09936v1},
	abstract = {Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.},
	language = {en},
	urldate = {2024-10-07},
	journal = {arXiv.org},
	author = {Song, Jiaming and Ren, Hongyu and Sadigh, Dorsa and Ermon, Stefano},
	month = jul,
	year = {2018},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	urldate = {2024-10-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year = {2017},
}

@misc{hejna_inverse_2023,
	title = {Inverse {Preference} {Learning}: {Preference}-based {RL} without a {Reward} {Function}},
	shorttitle = {Inverse {Preference} {Learning}},
	url = {https://arxiv.org/abs/2305.15363v2},
	abstract = {Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na{\textbackslash}"ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the \$Q\$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.},
	language = {en},
	urldate = {2024-10-03},
	journal = {arXiv.org},
	author = {Hejna, Joey and Sadigh, Dorsa},
	month = may,
	year = {2023},
}

@article{ziebart_maximum_nodate,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	language = {en},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
}

@inproceedings{yu_multi-agent_2019,
	title = {Multi-{Agent} {Adversarial} {Inverse} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v97/yu19e.html},
	abstract = {Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with the ground truth rewards, while significantly outperforms prior methods in terms of policy imitation.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Lantao and Song, Jiaming and Ermon, Stefano},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7194--7201},
}

@inproceedings{swamy_inverse_2023,
	title = {Inverse {Reinforcement} {Learning} without {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v202/swamy23a.html},
	abstract = {Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice, we find that we are able to significantly speed up the prior art on continuous control tasks.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Swamy, Gokul and Wu, David and Choudhury, Sanjiban and Bagnell, Drew and Wu, Steven},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {33299--33318},
}

@misc{fu_learning_2018,
	title = {Learning {Robust} {Rewards} with {Adversarial} {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1710.11248},
	doi = {10.48550/arXiv.1710.11248},
	abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv:1710.11248 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{hadfield-menell_cooperative_2016,
	title = {Cooperative {Inverse} {Reinforcement} {Learning}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html},
	abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human’s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
	urldate = {2024-10-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
	year = {2016},
}

@article{adams_survey_2022,
	title = {A survey of inverse reinforcement learning},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-021-10108-x},
	doi = {10.1007/s10462-021-10108-x},
	abstract = {Learning from demonstration, or imitation learning, is the process of learning to act in an environment from examples provided by a teacher. Inverse reinforcement learning (IRL) is a specific form of learning from demonstration that attempts to estimate the reward function of a Markov decision process from examples provided by the teacher. The reward function is often considered the most succinct description of a task. In simple applications, the reward function may be known or easily derived from properties of the system and hard coded into the learning process. However, in complex applications, this may not be possible, and it may be easier to learn the reward function by observing the actions of the teacher. This paper provides a comprehensive survey of the literature on IRL. This survey outlines the differences between IRL and two similar methods - apprenticeship learning and inverse optimal control. Further, this survey organizes the IRL literature based on the principal method, describes applications of IRL algorithms, and provides areas of future research.},
	language = {en},
	number = {6},
	urldate = {2024-10-01},
	journal = {Artificial Intelligence Review},
	author = {Adams, Stephen and Cody, Tyler and Beling, Peter A.},
	month = aug,
	year = {2022},
	keywords = {Apprenticeship learning, Artificial Intelligence, Inverse optimal control, Inverse reinforcement learning, Learning from demonstration, Reinforcement learning},
	pages = {4307--4346},
}

@article{arora_survey_2021,
	title = {A survey of inverse reinforcement learning: {Challenges}, methods and progress},
	volume = {297},
	issn = {0004-3702},
	shorttitle = {A survey of inverse reinforcement learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000515},
	doi = {10.1016/j.artint.2021.103500},
	abstract = {Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the extant literature in IRL, this article serves as a comprehensive reference for researchers and practitioners of machine learning as well as those new to it to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article surveys a vast collection of foundational methods grouped together by the commonality of their objectives, and elaborates how these methods mitigate the challenges. We further discuss extensions to the traditional IRL methods for handling imperfect perception, an incomplete model, learning multiple reward functions and nonlinear reward functions. The article concludes the survey with a discussion of some broad advances in the research area and currently open research questions.},
	urldate = {2024-10-01},
	journal = {Artificial Intelligence},
	author = {Arora, Saurabh and Doshi, Prashant},
	month = aug,
	year = {2021},
	keywords = {Generalization, Learning accuracy, Learning from demonstration, Reinforcement learning, Reward function, Survey},
	pages = {103500},
}

@inproceedings{iqbal_actor-attention-critic_2019,
	title = {Actor-{Attention}-{Critic} for {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v97/iqbal19a.html},
	abstract = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Iqbal, Shariq and Sha, Fei},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2961--2970},
}

@inproceedings{wang_competitive_2018,
	title = {Competitive {Multi}-agent {Inverse} {Reinforcement} {Learning} with {Sub}-optimal {Demonstrations}},
	url = {https://proceedings.mlr.press/v80/wang18d.html},
	abstract = {This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be suboptimal. Compared to previous works that decouple agents in the game by assuming optimality in expert policies, we introduce a new objective function that directly pits experts against Nash Equilibrium policies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. To ?nd Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to existing benchmark algorithms. Moreover, our algorithm successfully recovers reward and policy functions regardless of the quality of the sub-optimal expert demonstration set.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Xingyu and Klabjan, Diego},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5143--5151},
}

@article{liu_distributed_2022,
	title = {Distributed {Inverse} {Constrained} {Reinforcement} {Learning} for {Multi}-agent {Systems}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html},
	language = {en},
	urldate = {2024-10-01},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Shicheng and Zhu, Minghui},
	month = dec,
	year = {2022},
	pages = {33444--33456},
}

@inproceedings{natarajan_multi-agent_2010,
	title = {Multi-{Agent} {Inverse} {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/5708862},
	doi = {10.1109/ICMLA.2010.65},
	abstract = {Learning the reward function of an agent by observing its behavior is termed inverse reinforcement learning and has applications in learning from demonstration or apprenticeship learning. We introduce the problem of multi-agent inverse reinforcement learning, where reward functions of multiple agents are learned by observing their uncoordinated behavior. A centralized controller then learns to coordinate their behavior by optimizing a weighted sum of reward functions of all the agents. We evaluate our approach on a traffic-routing domain, in which a controller coordinates actions of multiple traffic signals to regulate traffic density. We show that the learner is not only able to match but even significantly outperform the expert.},
	urldate = {2024-10-01},
	booktitle = {2010 {Ninth} {International} {Conference} on {Machine} {Learning} and {Applications}},
	author = {Natarajan, Sriraam and Kunapuli, Gautam and Judah, Kshitij and Tadepalli, Prasad and Kersting, Kristian and Shavlik, Jude},
	month = dec,
	year = {2010},
	keywords = {Aerospace electronics, Equations, Learning, Mathematical model, Optimization, Reinforcement Learning, Road transportation, Trajectory},
	pages = {395--400},
}

@inproceedings{zhou_deep_2019,
	title = {Deep {Reinforcement} {Learning} {Based} {Intelligent} {Decision} {Making} for {Two}-player {Sequential} {Game} with {Uncertain} {Irrational} {Player}},
	url = {https://ieeexplore.ieee.org/abstract/document/9002811},
	doi = {10.1109/SSCI44817.2019.9002811},
	abstract = {In this paper, two player sequential game with an unknown non-stationary irrational player is investigated for cooperative autonomous robots decision making applications. In practice, the irrationality of agent can seriously degrade the effectiveness of decision making especially for distributed cooperative tasks with applications to multi-robot systems. Specifically, The irrationality can be caused by the cooperation agent's mechanical failure or sensor flaw. To handle this issue, a novel dynamic evaluation system, which includes two important parameters, i.e. cooperation index and competitive flag, is designed to efficiently quantify the player's level of cooperation or competition firstly. Then, the continuous deep Q network space is proposed to predict the action value with respect to a continuous cooperation index. Inspired from the framework of "Friend or Foe" algorithm, a novel hybrid online multi-agent deep reinforcement learning algorithm is proposed. The designed algorithm can evaluate the cooperator's cooperative level as well as maximize the total payoff by learning in a continuous deep Q network space. Eventually, numerical simulation and experimental tests are provided to demonstrate the effectiveness of the designed algorithm.},
	urldate = {2024-10-01},
	booktitle = {2019 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Zhou, Zejian and Xu, Hao},
	month = dec,
	year = {2019},
	keywords = {Decision making, Games, Heuristic algorithms, Indexes, Learning (artificial intelligence), Machine learning, Robots, deep reinforcement learning, intelligent decision making, two-player sequential game},
	pages = {9--15},
}

@article{unhelkar_human-aware_2018-1,
	title = {Human-{Aware} {Robotic} {Assistant} for {Collaborative} {Assembly}: {Integrating} {Human} {Motion} {Prediction} {With} {Planning} in {Time}},
	volume = {3},
	issn = {2377-3766},
	shorttitle = {Human-{Aware} {Robotic} {Assistant} for {Collaborative} {Assembly}},
	url = {https://ieeexplore.ieee.org/abstract/document/8307470?casa_token=Qr9vpon4eesAAAAA:ioiVGBXg2XPSXJi7Hr7lEUPYrgJbdG9PFgdcKuEBzI6G-GrH_wgpIoZ8BkIs0Wni1OEckxwSTgM},
	doi = {10.1109/LRA.2018.2812906},
	abstract = {Introducing mobile robots into the collaborative assembly process poses unique challenges for ensuring efficient and safe human-robot interaction. Current human-robot work cells require the robot to cease operating completely whenever a human enters a shared region of the given cell, and the robots do not explicitly model or adapt to the behavior of the human. In this work, we present a human-aware robotic system with single-axis mobility that incorporates both predictions of human motion and planning in time to execute efficient and safe motions during automotive final assembly. We evaluate our system in simulation against three alternative methods, including a baseline approach emulating the behavior of standard safety systems in factories today. We also assess the system within a factory test environment. Through both live demonstration and results from simulated experiments, we show that our approach produces statistically significant improvements in quantitative measures of safety and fluency of interaction.},
	number = {3},
	urldate = {2024-10-01},
	journal = {IEEE Robotics and Automation Letters},
	author = {Unhelkar, Vaibhav V. and Lasota, Przemyslaw A. and Tyroller, Quirin and Buhai, Rares-Darius and Marceau, Laurie and Deml, Barbara and Shah, Julie A.},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Collaboration, Engines, Physical human-robot interaction, Planning, Production facilities, Robots, Safety, Task analysis, assembly, collaborative robots},
	pages = {2394--2401},
}

@inproceedings{liu_heterogeneous_2022,
	title = {Heterogeneous {Skill} {Learning} for {Multi}-agent {Tasks}},
	url = {https://openreview.net/forum?id=5VCT-DptDTs},
	abstract = {Heterogeneous behaviours are widespread in many multi-agent tasks, which have not been paid much attention in the community of multi-agent reinforcement learning. It would be a key factor for improving the learning performance to efficiently characterize and automatically find heterogeneous behaviours. In this paper, we introduce the concept of the skill to explore the ability of heterogeneous behaviours. We propose a novel skill-based multi-agent reinforcement learning framework to enable agents to master diverse skills. Specifically, our framework consists of the skill representation mechanism, the skill selector and the skill-based policy learning mechanism. We design an auto-encoder model to generate the latent variable as the skill representation by incorporating the environment information, which ensures the distinguishable of agents for skill selection and the discriminability for the skill learning. With the representation, a skill selection mechanism is invented to realize the assignment from agents to skills. Meanwhile, diverse skill-based policies are generated through a novel skill-based policy learning method. To promote efficient skill discovery, a mutual information based intrinsic reward function is constructed. Empirical results show that our framework obtains the best performance on three challenging benchmarks, i.e., StarCraft II micromanagement tasks, Google Research Football and GoBigger, over state-of-the-art MARL methods.},
	language = {en},
	urldate = {2024-09-26},
	author = {Liu, Yuntao and Li, Yuan and Xu, Xinhai and Dou, Yong and Liu, Donghong},
	month = oct,
	year = {2022},
}

@article{even-dar_convergence_nodate,
	title = {Convergence of {Optimistic} and {Incremental} {Q}-{Learning}},
	abstract = {Vie sho,v the convergence of tV/O deterministic variants of Qlearning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an Eoptimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the E-greedy Q-learning.},
	language = {en},
	author = {Even-dar, Eyal and Mansour, Yishay},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lyu_contrasting_2021,
	title = {Contrasting {Centralized} and {Decentralized} {Critics} in {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2102.04402},
	doi = {10.48550/arXiv.2102.04402},
	abstract = {Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
	month = dec,
	year = {2021},
	note = {arXiv:2102.04402 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{matignon_hysteretic_2007,
	address = {San Diego, CA},
	title = {Hysteretic {Q}-learning : an algorithm for {Decentralized} {Reinforcement} {Learning} in {Cooperative} {Multi}-{Agent} {Teams}},
	isbn = {978-1-4244-0911-2},
	shorttitle = {Hysteretic {Q}-learning},
	url = {https://ieeexplore.ieee.org/document/4399095/},
	doi = {10.1109/IROS.2007.4399095},
	abstract = {Multi-agent systems (MAS) are a ﬁeld of study of growing interest in a variety of domains such as robotics or distributed controls. The article focuses on decentralized reinforcement learning (RL) in cooperative MAS, where a team of independent learning robots (IL) try to coordinate their individual behavior to reach a coherent joint behavior. We assume that each robot has no information about its teammates’ actions. To date, RL approaches for such ILs did not guarantee convergence to the optimal joint policy in scenarios where the coordination is difﬁcult. We report an investigation of existing algorithms for the learning of coordination in cooperative MAS, and suggest a Q-Learning extension for ILs, called Hysteretic Q-Learning. This algorithm does not require any additional communication between robots. Its advantages are showing off and compared to other methods on various applications : bimatrix games, collaborative ball balancing task and pursuit domain.},
	language = {en},
	urldate = {2024-07-26},
	booktitle = {2007 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Matignon, Laetitia and Laurent, Guillaume J. and Le Fort-Piat, Nadine},
	month = oct,
	year = {2007},
	pages = {64--69},
}

@misc{omidshafiei_deep_2017,
	title = {Deep {Decentralized} {Multi}-task {Multi}-{Agent} {Reinforcement} {Learning} under {Partial} {Observability}},
	url = {http://arxiv.org/abs/1703.06182},
	doi = {10.48550/arXiv.1703.06182},
	abstract = {Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.},
	urldate = {2024-07-26},
	publisher = {arXiv},
	author = {Omidshafiei, Shayegan and Pazis, Jason and Amato, Christopher and How, Jonathan P. and Vian, John},
	month = jul,
	year = {2017},
	note = {arXiv:1703.06182 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{schulman_trust_2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	doi = {10.48550/arXiv.1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv:1502.05477 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{sun_trust_2023,
	title = {Trust {Region} {Bounds} for {Decentralized} {PPO} {Under} {Non}-stationarity},
	url = {http://arxiv.org/abs/2202.00082},
	abstract = {We present trust region bounds for optimizing decentralized policies in cooperative Multi-Agent Reinforcement Learning (MARL), which holds even when the transition dynamics are non-stationary. This new analysis provides a theoretical understanding of the strong performance of two recent actor-critic methods for MARL, which both rely on independent ratios, i.e., computing probability ratios separately for each agent’s policy. We show that, despite the nonstationarity that independent ratios cause, a monotonic improvement guarantee still arises as a result of enforcing the trust region constraint over all decentralized policies. We also show this trust region constraint can be effectively enforced in a principled way by bounding independent ratios based on the number of agents in training, providing a theoretical foundation for proximal ratio clipping. Finally, our empirical results support the hypothesis that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized training, and tuning the hyperparameters with regards to the number of agents, as predicted by our theoretical analysis.},
	language = {en},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Sun, Mingfei and Devlin, Sam and Beck, Jacob and Hofmann, Katja and Whiteson, Shimon},
	month = feb,
	year = {2023},
	note = {arXiv:2202.00082 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chen_adaptive_2018,
	title = {An {Adaptive} {Clipping} {Approach} for {Proximal} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1804.06461},
	doi = {10.48550/arXiv.1804.06461},
	abstract = {Very recently proximal policy optimization (PPO) algorithms have been proposed as first-order optimization methods for effective reinforcement learning. While PPO is inspired by the same learning theory that justifies trust region policy optimization (TRPO), PPO substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of {\textbackslash}emph\{clipped policy optimization\} from sampled data. Although clipping in PPO stands for an important new mechanism for efficient and reliable policy update, it may fail to adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new surrogate learning objective featuring an adaptive clipping mechanism is proposed in this paper, enabling us to develop a new algorithm, known as PPO-\${\textbackslash}lambda\$. PPO-\${\textbackslash}lambda\$ optimizes policies repeatedly based on a theoretical target for adaptive policy improvement. Meanwhile, destructively large policy update can be effectively prevented through both clipping and adaptive control of a hyperparameter \${\textbackslash}lambda\$ in PPO-\${\textbackslash}lambda\$, ensuring high learning reliability. PPO-\${\textbackslash}lambda\$ enjoys the same simple and efficient design as PPO. Empirically on several Atari game playing tasks and benchmark control tasks, PPO-\${\textbackslash}lambda\$ also achieved clearly better performance than PPO.},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Chen, Gang and Peng, Yiming and Zhang, Mengjie},
	month = apr,
	year = {2018},
	note = {arXiv:1804.06461 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{son_qtran_2019,
	title = {{QTRAN}: {Learning} to {Factorize} with {Transformation} for {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {{QTRAN}},
	url = {http://arxiv.org/abs/1905.05408},
	abstract = {We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint actionvalue function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modiﬁed predator-prey demonstrate QTRAN’s superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.},
	language = {en},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
	month = may,
	year = {2019},
	note = {arXiv:1905.05408 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@inproceedings{zhang_fop_2021,
	title = {{FOP}: {Factorizing} {Optimal} {Joint} {Policy} of {Maximum}-{Entropy} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {{FOP}},
	url = {https://proceedings.mlr.press/v139/zhang21m.html},
	abstract = {Value decomposition recently injects vigorous vitality into multi-agent actor-critic methods. However, existing decomposed actor-critic methods cannot guarantee the convergence of global optimum. In this paper, we present a novel multi-agent actor-critic method, FOP, which can factorize the optimal joint policy induced by maximum-entropy multi-agent reinforcement learning (MARL) into individual policies. Theoretically, we prove that factorized individual policies of FOP converge to the global optimum. Empirically, in the well-known matrix game and differential game, we verify that FOP can converge to the global optimum for both discrete and continuous action spaces. We also evaluate FOP on a set of StarCraft II micromanagement tasks, and demonstrate that FOP substantially outperforms state-of-the-art decomposed value-based and actor-critic methods.},
	language = {en},
	urldate = {2024-06-10},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Tianhao and Li, Yueheng and Wang, Chen and Xie, Guangming and Lu, Zongqing},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12491--12500},
}

@inproceedings{wang_trust_2019,
	title = {Trust {Region}-{Guided} {Proximal} {Policy} {Optimization}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a666587afda6e89aec274a3657558a27-Abstract.html},
	abstract = {Proximal policy optimization (PPO) is one of the most popular deep reinforcement learning (RL) methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, as a model-free RL method, the success of PPO relies heavily on the effectiveness of its exploratory policy search. In this paper, we give an in-depth analysis on the exploration behavior of PPO, and show that PPO is prone to suffer from the risk of lack of exploration especially under the case of bad initialization, which may lead to the failure of training or being trapped in bad local optima. To address these issues, we proposed a novel policy optimization method, named Trust Region-Guided PPO (TRGPPO), which adaptively adjusts the clipping range within the trust region. We formally show that this method not only improves the exploration ability within the trust region but enjoys a better performance bound compared to the original PPO as well. Extensive experiments verify the advantage of the proposed method.},
	urldate = {2024-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Yuhui and He, Hao and Tan, Xiaoyang and Gan, Yaozhong},
	year = {2019},
}

@misc{kuba_trust_2022,
	title = {Trust {Region} {Policy} {Optimisation} in {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2109.11251},
	doi = {10.48550/arXiv.2109.11251},
	abstract = {Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL), the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, therefore establishing a new state of the art.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Kuba, Jakub Grudzien and Chen, Ruiqing and Wen, Muning and Wen, Ying and Sun, Fanglei and Wang, Jun and Yang, Yaodong},
	month = apr,
	year = {2022},
	note = {arXiv:2109.11251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{yu_surprising_2022,
	title = {The {Surprising} {Effectiveness} of {PPO} in {Cooperative}, {Multi}-{Agent} {Games}},
	url = {http://arxiv.org/abs/2103.01955},
	doi = {10.48550/arXiv.2103.01955},
	abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at {\textbackslash}url\{https://github.com/marlbenchmark/on-policy\}.},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
	month = nov,
	year = {2022},
	note = {arXiv:2103.01955 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{papoudakis_benchmarking_2021,
	title = {Benchmarking {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Algorithms} in {Cooperative} {Tasks}},
	url = {http://arxiv.org/abs/2006.07869},
	doi = {10.48550/arXiv.2006.07869},
	abstract = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Papoudakis, Georgios and Christianos, Filippos and Schäfer, Lukas and Albrecht, Stefano V.},
	month = nov,
	year = {2021},
	note = {arXiv:2006.07869 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{yu_surprising_2022-1,
	title = {The {Surprising} {Effectiveness} of {PPO} in {Cooperative}, {Multi}-{Agent} {Games}},
	url = {http://arxiv.org/abs/2103.01955},
	abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is signiﬁcantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is signiﬁcantly less sample efﬁcient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-speciﬁc algorithmic modiﬁcations or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both ﬁnal returns and sample efﬁciency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO’s empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
	month = nov,
	year = {2022},
	note = {arXiv:2103.01955 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@inproceedings{wu_coordinated_2021,
	title = {Coordinated {Proximal} {Policy} {Optimization}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/de73998802680548b916f1947ffbad76-Abstract.html},
	abstract = {We present Coordinated Proximal Policy Optimization (CoPPO), an algorithm that extends the original Proximal Policy Optimization (PPO) to the multi-agent setting. The key idea lies in the coordinated adaptation of step size during the policy update process among multiple agents. We prove the monotonicity of policy improvement when optimizing a theoretically-grounded joint objective, and derive a simplified optimization objective based on a set of approximations. We then interpret that such an objective in CoPPO can achieve dynamic credit assignment among agents, thereby alleviating the high variance issue during the concurrent update of agent policies. Finally, we demonstrate that CoPPO outperforms several strong baselines and is competitive with the latest multi-agent PPO method (i.e. MAPPO) under typical multi-agent settings, including cooperative matrix games and the StarCraft II micromanagement tasks.},
	urldate = {2024-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Zifan and Yu, Chao and Ye, Deheng and Zhang, Junge and piao, haiyin and Zhuo, Hankz Hankui},
	year = {2021},
	pages = {26437--26448},
}

@misc{foerster_learning_2018,
	title = {Learning with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/1709.04326},
	doi = {10.48550/arXiv.1709.04326},
	abstract = {Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Foerster, Jakob N. and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
	month = sep,
	year = {2018},
	note = {arXiv:1709.04326 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
}

@article{conitzer_foundations_2023,
	title = {Foundations of {Cooperative} {AI}},
	volume = {37},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26791},
	doi = {10.1609/aaai.v37i13.26791},
	abstract = {AI systems can interact in unexpected ways, sometimes with disastrous consequences. As AI gets to control more of our world, these interactions will become more common and have higher stakes. As AI becomes more advanced, these interactions will become more sophisticated, and game theory will provide the tools for analyzing these interactions. However, AI agents are in some ways unlike the agents traditionally studied in game theory, introducing new challenges as well as opportunities. We propose a research agenda to develop the game theory of highly advanced AI agents, with a focus on achieving cooperation.},
	language = {en},
	number = {13},
	urldate = {2024-04-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Conitzer, Vincent and Oesterheld, Caspar},
	month = jun,
	year = {2023},
	pages = {15359--15367},
}

@article{amato_planning_nodate,
	title = {Planning with {Macro}-{Actions} in {Decentralized} {POMDPs}},
	abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for decentralized decision making under uncertainty. However, they typically model a problem at a low level of granularity, where each agent’s actions are primitive operations lasting exactly one time step. We address the case where each agent has macroactions: temporally extended actions which may require diﬀerent amounts of time to execute. We model macroactions as options in a factored Dec-POMDP model, focusing on options which depend only on information available to an individual agent while executing. This enables us to model systems where coordination decisions only occur at the level of deciding which macro-actions to execute, and the macro-actions themselves can then be executed to completion. The core technical diﬃculty when using options in a Dec-POMDP is that the options chosen by the agents no longer terminate at the same time. We present extensions of two leading Dec-POMDP algorithms for generating a policy with options and discuss the resulting form of optimality. Our results show that these algorithms retain agent coordination while allowing near-optimal solutions to be generated for signiﬁcantly longer horizons and larger state-spaces than previous Dec-POMDP methods.},
	language = {en},
	author = {Amato, Christopher and Konidaris, George D and Kaelbling, Leslie P},
}

@inproceedings{omidshafiei_graph-based_2016,
	title = {Graph-based {Cross} {Entropy} method for solving multi-robot decentralized {POMDPs}},
	url = {https://ieeexplore.ieee.org/document/7487751},
	doi = {10.1109/ICRA.2016.7487751},
	abstract = {This paper introduces a probabilistic algorithm for multi-robot decision-making under uncertainty, which can be posed as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP). Dec-POMDPs are inherently synchronous decision-making frameworks which require significant computational resources to be solved, making them infeasible for many real-world robotics applications. The Decentralized Partially Observable Semi-Markov Decision Process (Dec-POSMDP) was recently introduced as an extension of the Dec-POMDP that uses high-level macro-actions to allow large-scale, asynchronous decision-making. However, existing Dec-POSMDP solution methods have limited scalability or perform poorly as the problem size grows. This paper proposes a cross-entropy based Dec-POSMDP algorithm motivated by the combinatorial optimization literature. The algorithm is applied to a constrained package delivery domain, where it significantly outperforms existing Dec-POSMDP solution methods.},
	urldate = {2024-04-23},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar and Amato, Christopher and Liu, Shih-Yuan and How, Jonathan P. and Vian, John},
	month = may,
	year = {2016},
	keywords = {Decision making, History, Probabilistic logic, Robot sensing systems, Synchronization, Uncertainty},
	pages = {5395--5402},
}

@inproceedings{makar_hierarchical_2001,
	address = {New York, NY, USA},
	series = {{AGENTS} '01},
	title = {Hierarchical multi-agent reinforcement learning},
	isbn = {978-1-58113-326-4},
	url = {https://dl.acm.org/doi/10.1145/375735.376302},
	doi = {10.1145/375735.376302},
	abstract = {In this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi-agent tasks. We extend the MAXQ framework to the multi-agent case. Each agent uses the same MAXQ hierarchy to decompose a task into sub-tasks. Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents. Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy. The Q nodes at the highest level(s) of the hierarchy are configured to represent the joint task-action space among multiple agents. In this approach, each agent only knows what other agents are doing at the level of sub-tasks, and is unaware of lower level (primitive) actions. This hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub-tasks, rather than attempting to learn coordination taking into account primitive joint state-action values. We apply this hierarchical multi-agent reinforcement learning algorithm to a complex AGV scheduling task and compare its performance and speed with other learning approaches, including flat multi-agent, single agent using MAXQ, selfish multiple agents using MAXQ (where each agent acts independently without communicating with the other agents), as well as several well-known AGV heuristics like "first come first serve", "highest queue first" and "nearest station first". We also compare the tradeoffs in learning speed vs. performance of modeling joint action values at multiple levels in the MAXQ hierarchy.},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the fifth international conference on {Autonomous} agents},
	publisher = {Association for Computing Machinery},
	author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
	month = may,
	year = {2001},
	pages = {246--253},
}

@misc{xu_haven_2022,
	title = {{HAVEN}: {Hierarchical} {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning} with {Dual} {Coordination} {Mechanism}},
	shorttitle = {{HAVEN}},
	url = {http://arxiv.org/abs/2110.07246},
	doi = {10.48550/arXiv.2110.07246},
	abstract = {Recently, some challenging tasks in multi-agent systems have been solved by some hierarchical reinforcement learning methods. Inspired by the intra-level and inter-level coordination in the human nervous system, we propose a novel value decomposition framework HAVEN based on hierarchical reinforcement learning for fully cooperative multi-agent problems. To address the instability arising from the concurrent optimization of policies between various levels and agents, we introduce the dual coordination mechanism of inter-level and inter-agent strategies by designing reward functions in a two-level hierarchy. HAVEN does not require domain knowledge and pre-training, and can be applied to any value decomposition variant. Our method achieves desirable results on different decentralized partially observable Markov decision process domains and outperforms other popular multi-agent hierarchical reinforcement learning algorithms.},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Xu, Zhiwei and Bai, Yunpeng and Zhang, Bin and Li, Dapeng and Fan, Guoliang},
	month = dec,
	year = {2022},
	note = {arXiv:2110.07246 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@incollection{han_multi-agent_2019,
	title = {Multi-agent {Hierarchical} {Reinforcement} {Learning} with {Dynamic} {Termination}},
	volume = {11671},
	url = {http://arxiv.org/abs/1910.09508},
	abstract = {In a multi-agent system, an agent's optimal policy will typically depend on the policies chosen by others. Therefore, a key issue in multi-agent systems research is that of predicting the behaviours of others, and responding promptly to changes in such behaviours. One obvious possibility is for each agent to broadcast their current intention, for example, the currently executed option in a hierarchical reinforcement learning framework. However, this approach results in inflexibility of agents if options have an extended duration and are dynamic. While adjusting the executed option at each step improves flexibility from a single-agent perspective, frequent changes in options can induce inconsistency between an agent's actual behaviour and its broadcast intention. In order to balance flexibility and predictability, we propose a dynamic termination Bellman equation that allows the agents to flexibly terminate their options. We evaluate our model empirically on a set of multi-agent pursuit and taxi tasks, and show that our agents learn to adapt flexibly across scenarios that require different termination behaviours.},
	urldate = {2024-04-23},
	author = {Han, Dongge and Boehmer, Wendelin and Wooldridge, Michael and Rogers, Alex},
	year = {2019},
	doi = {10.1007/978-3-030-29911-8_7},
	note = {arXiv:1910.09508 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	pages = {80--92},
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	language = {en},
	number = {1},
	urldate = {2024-04-23},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	keywords = {Incremental learning, connectionism, credit assignment, evaluation functions, prediction},
	pages = {9--44},
}

@article{sutton_learning_1988-1,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	number = {1},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	pages = {9--44},
}

@misc{amato_planning_2014,
	title = {Planning for {Decentralized} {Control} of {Multiple} {Robots} {Under} {Uncertainty}},
	url = {http://arxiv.org/abs/1402.2871},
	doi = {10.48550/arXiv.1402.2871},
	abstract = {We describe a probabilistic framework for synthesizing control policies for general multi-robot systems, given environment and sensor models and a cost function. Decentralized, partially observable Markov decision processes (Dec-POMDPs) are a general model of decision processes where a team of agents must cooperate to optimize some objective (specified by a shared reward or cost function) in the presence of uncertainty, but where communication limitations mean that the agents cannot share their state, so execution must proceed in a decentralized fashion. While Dec-POMDPs are typically intractable to solve for real-world problems, recent research on the use of macro-actions in Dec-POMDPs has significantly increased the size of problem that can be practically solved as a Dec-POMDP. We describe this general model, and show how, in contrast to most existing methods that are specialized to a particular problem class, it can synthesize control policies that use whatever opportunities for coordination are present in the problem, while balancing off uncertainty in outcomes, sensor information, and information about other agents. We use three variations on a warehouse task to show that a single planner of this type can generate cooperative behavior using task allocation, direct communication, and signaling, as appropriate.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Amato, Christopher and Konidaris, George D. and Cruz, Gabriel and Maynor, Christopher A. and How, Jonathan P. and Kaelbling, Leslie P.},
	month = feb,
	year = {2014},
	note = {arXiv:1402.2871 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Robotics, I.2.11, I.2.9},
}

@article{amato_policy_nodate,
	title = {Policy {Search} for {Multi}-{Robot} {Coordination} under {Uncertainty}},
	abstract = {We introduce a principled method for multi-robot coordination based on a generic model (termed a MacDecPOMDP) of multi-robot cooperative planning in the presence of stochasticity, uncertain sensing and communication limitations. We present a new MacDec-POMDP planning algorithm that searches over policies represented as ﬁnite-state controllers, rather than the existing policy tree representation. Finite-state controllers can be much more concise than trees, are much easier to interpret, and can operate over an inﬁnite horizon. The resulting policy search algorithm requires a substantially simpler simulator that models only the outcomes of executing a given set of motor controllers, not the details of the executions themselves and can to solve signiﬁcantly larger problems than existing MacDec-POMDP planners. We demonstrate signiﬁcantly improved performance over previous methods and application to a cooperative multi-robot bartending task, showing that our method can be used for actual multi-robot systems.},
	language = {en},
	author = {Amato, Christopher and Konidaris, George and Anders, Ariel and Cruz, Gabriel and How, Jonathan P and Kaelbling, Leslie P},
}

@book{oliehoek_concise_2016,
	address = {Cham},
	series = {{SpringerBriefs} in {Intelligent} {Systems}},
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs}},
	isbn = {978-3-319-28927-4 978-3-319-28929-8},
	url = {http://link.springer.com/10.1007/978-3-319-28929-8},
	urldate = {2024-04-16},
	publisher = {Springer International Publishing},
	author = {Oliehoek, Frans A. and Amato, Christopher},
	year = {2016},
	doi = {10.1007/978-3-319-28929-8},
}

@book{oliehoek_concise_2016-1,
	address = {Cham},
	series = {{SpringerBriefs} in {Intelligent} {Systems}},
	title = {A {Concise} {Introduction} to {Decentralized} {POMDPs}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-28927-4 978-3-319-28929-8},
	url = {http://link.springer.com/10.1007/978-3-319-28929-8},
	language = {en},
	urldate = {2024-04-16},
	publisher = {Springer International Publishing},
	author = {Oliehoek, Frans A. and Amato, Christopher},
	year = {2016},
	doi = {10.1007/978-3-319-28929-8},
}

@article{li_robust_2019,
	title = {Robust {Multi}-{Agent} {Reinforcement} {Learning} via {Minimax} {Deep} {Deterministic} {Policy} {Gradient}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4327},
	doi = {10.1609/aaai.v33i01.33014213},
	abstract = {Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.},
	language = {en},
	number = {01},
	urldate = {2024-04-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4213--4220},
}

@misc{foerster_counterfactual_2017,
	title = {Counterfactual {Multi}-{Agent} {Policy} {Gradients}},
	url = {http://arxiv.org/abs/1705.08926},
	doi = {10.48550/arXiv.1705.08926},
	abstract = {Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	month = dec,
	year = {2017},
	note = {arXiv:1705.08926 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@inproceedings{rashid_weighted_2020,
	title = {Weighted {QMIX}: {Expanding} {Monotonic} {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {33},
	shorttitle = {Weighted {QMIX}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/73a427badebe0e32caa2e1fc7530b7f3-Abstract.html},
	urldate = {2024-04-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rashid, Tabish and Farquhar, Gregory and Peng, Bei and Whiteson, Shimon},
	year = {2020},
	pages = {10199--10210},
}

@article{du_liir_nodate,
	title = {{LIIR}: {Learning} {Individual} {Intrinsic} {Reward} in {Multi}-{Agent} {Reinforcement} {Learning}},
	abstract = {A great challenge in cooperative decentralized multi-agent reinforcement learning (MARL) is generating diversiﬁed behaviors for each individual agent when receiving only a team reward. Prior studies have paid many efforts on reward shaping or designing a centralized critic that can discriminatively credit the agents. In this paper, we propose to merge the two directions and learn each agent an intrinsic reward function which diversely stimulates the agents at each time step. Speciﬁcally, the intrinsic reward for a speciﬁc agent will be involved in computing a distinct proxy critic for the agent to direct the updating of its individual policy. Meanwhile, the parameterized intrinsic reward function will be updated towards maximizing the expected accumulated team reward from the environment so that the objective is consistent with the original MARL problem. The proposed method is referred to as learning individual intrinsic reward (LIIR) in MARL. We compare LIIR with a number of state-of-the-art MARL methods on battle games in StarCraft II. The results demonstrate the effectiveness of LIIR, and we show LIIR can assign each individual agent an insightful intrinsic reward per time step.},
	language = {en},
	author = {Du, Yali and Han, Lei and Fang, Meng and Liu, Ji and Dai, Tianhong and Tao, Dacheng},
}

@misc{iqbal_actor-attention-critic_2019,
	title = {Actor-{Attention}-{Critic} for {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1810.02912},
	abstract = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in singleagent settings. We present an actor-critic algorithm that trains decentralized policies in multiagent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multiagent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is ﬂexible enough to be applied to most multi-agent learning problems.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Iqbal, Shariq and Sha, Fei},
	month = may,
	year = {2019},
	note = {arXiv:1810.02912 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@inproceedings{noauthor_agent-oriented_2024,
	title = {Agent-{Oriented} {Centralized} {Critic} for {Asynchronous} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=qfAY7DoJaD},
	abstract = {Multi-agent reinforcement learning (MARL) has been actively developed and successfully applied in various fields. In the conventional MARL setting, which most previous works consider, all agents simultaneously take their actions every time due to the same duration across actions. However, real-world scenarios often involve agents executing actions with different duration resulting in asynchronous action selection across the agents. The macro-action decentralized partially observable Markov decision process (MacDec-POMDP) provides a framework for modeling multi-agent decision-making, where the action selection among the agents occurs asynchronously across time. While several works have explored MARL methods for MacDec-POMDP, existing methods for such asynchronicity focused on how to utilize trajectories for training and simply adopt conventional MARL architectures. In this paper, we propose a novel approach named agent-oriented centralized critic (AOCC) for MacDec-POMDP, which 1) explicitly encode each agent's observation history with the timestep information when the agent start to perform a macro-action, and 2) explicitly aggregating them for agent-oriented critic learning. Our experimental evaluation on a macro-action-based multi-agent benchmark demonstrates that the proposed approach significantly outperforms other baseline methods for MacDec-POMDP.},
	language = {en},
	urldate = {2024-04-08},
	month = mar,
	year = {2024},
}

@article{jiang_i2q_nodate,
	title = {{I2Q}: {A} {Fully} {Decentralized} {Q}-{Learning} {Algorithm}},
	abstract = {Fully decentralized multi-agent reinforcement learning has shown great potential for many real-world cooperative tasks, where the global information, e.g., the actions of other agents, is not accessible. Although independent Q-learning is widely used for decentralized training, the transition probabilities are non-stationary since other agents are updating policies simultaneously, which leads to non-guaranteed convergence of independent Q-learning. To deal with non-stationarity, we first introduce stationary ideal transition probabilities, on which independent Q-learning could converge to the global optimum. Further, we propose a fully decentralized method, I2Q, which performs independent Q-learning on the modeled ideal transition function to reach the global optimum. The modeling of ideal transition function in I2Q is fully decentralized and independent from the learned policies of other agents, helping I2Q be free from non-stationarity and learn the optimal policy. Empirically, we show that I2Q can achieve remarkable improvement in a variety of cooperative multi-agent tasks.},
	language = {en},
	author = {Jiang, Jiechuan and Lu, Zongqing},
}

@misc{lee_magic_2021,
	title = {{MAGIC}: {Learning} {Macro}-{Actions} for {Online} {POMDP} {Planning}},
	shorttitle = {{MAGIC}},
	url = {http://arxiv.org/abs/2011.03813},
	doi = {10.48550/arXiv.2011.03813},
	abstract = {The partially observable Markov decision process (POMDP) is a principled general framework for robot decision making under uncertainty, but POMDP planning suffers from high computational complexity, when long-term planning is required. While temporally-extended macro-actions help to cut down the effective planning horizon and significantly improve computational efficiency, how do we acquire good macro-actions? This paper proposes Macro-Action Generator-Critic (MAGIC), which performs offline learning of macro-actions optimized for online POMDP planning. Specifically, MAGIC learns a macro-action generator end-to-end, using an online planner's performance as the feedback. During online planning, the generator generates on the fly situation-aware macro-actions conditioned on the robot's belief and the environment context. We evaluated MAGIC on several long-horizon planning tasks both in simulation and on a real robot. The experimental results show that the learned macro-actions offer significant benefits in online planning performance, compared with primitive actions and handcrafted macro-actions.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Lee, Yiyuan and Cai, Panpan and Hsu, David},
	month = jul,
	year = {2021},
	note = {arXiv:2011.03813 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{de_witt_is_2020,
	title = {Is {Independent} {Learning} {All} {You} {Need} in the {StarCraft} {Multi}-{Agent} {Challenge}?},
	url = {http://arxiv.org/abs/2011.09533},
	doi = {10.48550/arXiv.2011.09533},
	abstract = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the {\textbackslash}emph\{centralized training with decentralized execution\} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip H. S. and Sun, Mingfei and Whiteson, Shimon},
	month = nov,
	year = {2020},
	note = {arXiv:2011.09533 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{oroojlooy_review_2023,
	title = {A review of cooperative multi-agent deep reinforcement learning},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04105-y},
	doi = {10.1007/s10489-022-04105-y},
	abstract = {Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.},
	language = {en},
	number = {11},
	urldate = {2024-04-08},
	journal = {Applied Intelligence},
	author = {Oroojlooy, Afshin and Hajinezhad, Davood},
	month = jun,
	year = {2023},
	pages = {13677--13722},
}

@misc{noauthor_notitle_nodate,
}

@article{zhong_heterogeneous-agent_nodate,
	title = {Heterogeneous-{Agent} {Reinforcement} {Learning}},
	language = {en},
	author = {Zhong, Yifan and Kuba, Jakub Grudzien and Feng, Xidong and Hu, Siyi and Ji, Jiaming and Yang, Yaodong},
}

@article{oroojlooy_review_2023-1,
	title = {A review of cooperative multi-agent deep reinforcement learning},
	volume = {53},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-022-04105-y},
	doi = {10.1007/s10489-022-04105-y},
	abstract = {Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.},
	language = {en},
	number = {11},
	urldate = {2024-03-28},
	journal = {Applied Intelligence},
	author = {Oroojlooy, Afshin and Hajinezhad, Davood},
	month = jun,
	year = {2023},
	keywords = {Cooperative learning, Multi-agent systems, Reinforcement learning},
	pages = {13677--13722},
}

@misc{eysenbach_diversity_2018,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	shorttitle = {Diversity is {All} {You} {Need}},
	url = {http://arxiv.org/abs/1802.06070},
	doi = {10.48550/arXiv.1802.06070},
	abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	month = oct,
	year = {2018},
	note = {arXiv:1802.06070 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{fox_multi-level_2017,
	title = {Multi-{Level} {Discovery} of {Deep} {Options}},
	url = {http://arxiv.org/abs/1703.08294},
	doi = {10.48550/arXiv.1703.08294},
	abstract = {Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72\% accuracy.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Fox, Roy and Krishnan, Sanjay and Stoica, Ion and Goldberg, Ken},
	month = oct,
	year = {2017},
	note = {arXiv:1703.08294 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chakravorty_option-critic_2020,
	title = {Option-{Critic} in {Cooperative} {Multi}-agent {Systems}},
	url = {http://arxiv.org/abs/1911.12825},
	doi = {10.48550/arXiv.1911.12825},
	abstract = {In this paper, we investigate learning temporal abstractions in cooperative multi-agent systems, using the options framework (Sutton et al, 1999). First, we address the planning problem for the decentralized POMDP represented by the multi-agent system, by introducing a {\textbackslash}emph\{common information approach\}. We use the notion of {\textbackslash}emph\{common beliefs\} and broadcasting to solve an equivalent centralized POMDP problem. Then, we propose the Distributed Option Critic (DOC) algorithm, which uses centralized option evaluation and decentralized intra-option improvement. We theoretically analyze the asymptotic convergence of DOC and build a new multi-agent environment to demonstrate its validity. Our experiments empirically show that DOC performs competitively against baselines and scales with the number of agents.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Chakravorty, Jhelum and Ward, Nadeem and Roy, Julien and Chevalier-Boisvert, Maxime and Basu, Sumana and Lupu, Andrei and Precup, Doina},
	month = mar,
	year = {2020},
	note = {arXiv:1911.12825 [cs, eess, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@misc{zhang_dac_2019,
	title = {{DAC}: {The} {Double} {Actor}-{Critic} {Architecture} for {Learning} {Options}},
	shorttitle = {{DAC}},
	url = {http://arxiv.org/abs/1904.12691},
	doi = {10.48550/arXiv.1904.12691},
	abstract = {We reformulate the option framework as two parallel augmented MDPs. Under this novel formulation, all policy optimization algorithms can be used off the shelf to learn intra-option policies, option termination conditions, and a master policy over options. We apply an actor-critic algorithm on each augmented MDP, yielding the Double Actor-Critic (DAC) architecture. Furthermore, we show that, when state-value functions are used as critics, one critic can be expressed in terms of the other, and hence only one critic is necessary. We conduct an empirical study on challenging robot simulation tasks. In a transfer learning setting, DAC outperforms both its hierarchy-free counterpart and previous gradient-based option learning algorithms.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Zhang, Shangtong and Whiteson, Shimon},
	month = sep,
	year = {2019},
	note = {arXiv:1904.12691 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{torreno_cooperative_2018,
	title = {Cooperative {Multi}-{Agent} {Planning}: {A} {Survey}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Cooperative {Multi}-{Agent} {Planning}},
	url = {http://arxiv.org/abs/1711.09057},
	doi = {10.1145/3128584},
	abstract = {Cooperative multi-agent planning (MAP) is a relatively recent research field that combines technologies, algorithms and techniques developed by the Artificial Intelligence Planning and Multi-Agent Systems communities. While planning has been generally treated as a single-agent task, MAP generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group. This paper reviews the most relevant approaches to MAP, putting the focus on the solvers that took part in the 2015 Competition of Distributed and Multi-Agent Planning, and classifies them according to their key features and relative performance.},
	number = {6},
	urldate = {2024-03-21},
	journal = {ACM Computing Surveys},
	author = {Torreño, Alejandro and Onaindia, Eva and Komenda, Antonín and Štolba, Michal},
	month = nov,
	year = {2018},
	note = {arXiv:1711.09057 [cs]},
	keywords = {68-42, 68-20, 68-35, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	pages = {1--32},
}

@article{jinnai_exploration_2020,
	title = {{EXPLORATION} {IN} {REINFORCEMENT} {LEARNING} {WITH} {DEEP} {COVERING} {OPTIONS}},
	abstract = {While many option discovery methods have been proposed to accelerate exploration in reinforcement learning, they are often heuristic. Recently, covering options was proposed to discover a set of options that provably reduce the upper bound of the environment’s cover time, a measure of the difﬁculty of exploration. However, they are constrained to tabular tasks and are not applicable to tasks with large or continuous state-spaces. We introduce deep covering options, an online method that extends covering options to large state spaces, automatically discovering taskagnostic options that encourage exploration. We evaluate our method in several challenging sparse-reward domains and we show that our approach identiﬁes less explored regions of the state-space and successfully generates options to visit these regions, substantially improving both the exploration and the total accumulated reward.},
	language = {en},
	author = {Jinnai, Yuu and Park, Jee Won and Machado, Marlos C and Konidaris, George},
	year = {2020},
}

@article{jin_creativity_2022,
	title = {Creativity of {AI}: {Automatic} {Symbolic} {Option} {Discovery} for {Facilitating} {Deep} {Reinforcement} {Learning}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Creativity of {AI}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20663},
	doi = {10.1609/aaai.v36i6.20663},
	abstract = {Despite of achieving great success in real life, Deep Reinforcement Learning (DRL) is still suffering from three critical issues, which are data efficiency, lack of the interpretability and transferability. Recent research shows that embedding symbolic knowledge into DRL is promising in addressing those challenges. Inspired by this, we introduce a novel deep reinforcement learning framework with symbolic options. This framework features a loop training procedure, which enables guiding the improvement of policy by planning with action models and symbolic options learned from interactive trajectories automatically. The learned symbolic options help doing the dense requirement of expert domain knowledge and provide inherent interpretabiliy of policies. Moreover, the transferability and data efficiency can be further improved by planning with the action models. To validate the effectiveness of this framework, we conduct experiments on two domains, Montezuma's Revenge and Office World respectively, and the results demonstrate the comparable performance, improved data efficiency, interpretability and transferability.},
	language = {en},
	number = {6},
	urldate = {2024-03-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jin, Mu and Ma, Zhihao and Jin, Kebing and Zhuo, Hankz Hankui and Chen, Chen and Yu, Chao},
	month = jun,
	year = {2022},
	note = {Number: 6},
	keywords = {And Scheduling (PRS), Planning, Routing},
	pages = {7042--7050},
}

@misc{zhang_hierarchical_2022,
	title = {Hierarchical {Reinforcement} {Learning} {By} {Discovering} {Intrinsic} {Options}},
	url = {http://arxiv.org/abs/2101.06521},
	doi = {10.48550/arXiv.2101.06521},
	abstract = {We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower-level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Zhang, Jesse and Yu, Haonan and Xu, Wei},
	month = aug,
	year = {2022},
	note = {arXiv:2101.06521 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{yang_hierarchical_2020,
	title = {Hierarchical {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning} with {Skill} {Discovery}},
	url = {http://arxiv.org/abs/1912.03558},
	doi = {10.48550/arXiv.1912.03558},
	abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
	urldate = {2024-03-11},
	author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
	month = may,
	year = {2020},
	note = {arXiv:1912.03558 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{bacon_option-critic_2016,
	title = {The {Option}-{Critic} {Architecture}},
	url = {http://arxiv.org/abs/1609.05140},
	doi = {10.48550/arXiv.1609.05140},
	abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	month = dec,
	year = {2016},
	note = {arXiv:1609.05140 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{konidaris_skill_nodate,
	title = {Skill {Discovery} in {Continuous} {Reinforcement} {Learning} {Domains} using {Skill} {Chaining}},
	abstract = {We introduce a skill discovery method for reinforcement learning in continuous domains that constructs chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates appropriate skills and achieves performance beneﬁts in a challenging continuous domain.},
	language = {en},
	author = {Konidaris, George and Barreto, Andre S},
}

@misc{chuck_hypothesis-driven_2020,
	title = {Hypothesis-{Driven} {Skill} {Discovery} for {Hierarchical} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1906.01408},
	abstract = {Deep reinforcement learning (DRL) is capable of learning high-performing policies on a variety of complex highdimensional tasks, ranging from video games to robotic manipulation. However, standard DRL methods often suffer from poor sample efﬁciency, partially because they aim to be entirely problem-agnostic. In this work, we introduce a novel approach to exploration and hierarchical skill learning that derives its sample efﬁciency from intuitive assumptions it makes about the behavior of objects both in the physical world and simulations which mimic physics. Speciﬁcally, we propose the Hypothesis Proposal and Evaluation (HyPE) algorithm, which discovers objects from raw pixel data, generates hypotheses about the controllability of observed changes in object state, and learns a hierarchy of skills to test these hypotheses. We demonstrate that HyPE can dramatically improve the sample efﬁciency of policy learning in two different domains: a simulated robotic blockpushing domain, and a popular benchmark task: Breakout. In these domains, HyPE learns high-scoring policies an order of magnitude faster than several state-of-the-art reinforcement learning methods.},
	language = {en},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Chuck, Caleb and Chockchowwat, Supawit and Niekum, Scott},
	month = mar,
	year = {2020},
	note = {arXiv:1906.01408 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_httpsarxivorgpdf190601408pdf_nodate,
	title = {https://arxiv.org/pdf/1906.01408.pdf},
	url = {https://arxiv.org/pdf/1906.01408.pdf},
	urldate = {2024-03-06},
}

@inproceedings{farahani_constructing_2018,
	title = {Constructing and {Evaluating} {Options} in {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8661047},
	doi = {10.1109/IS℡.2018.8661047},
	abstract = {In this paper, we propose a new subgoal based method for automatic construction of useful options. In our proposed method, subgoals are considered as border states of communities of the transition graph created after some initial agent interactions with the environment. We present a new community detection algorithm to provide an appropriate partitioning of the transition graph. Macro-actions are constructed for taking the agent from one community to other communities. In addition, we attempt to capture intuitions about features of useful macro-actions. There is a lack of a generic evaluation mechanism for evaluating each macro-action in previous research. We will propose a method for evaluating each macro-action separately. Inappropriate macro-actions are identified with this method and discarded from agent choices. Experimental results show a significant improvement in results after pruning macro-actions.},
	urldate = {2024-02-21},
	booktitle = {2018 9th {International} {Symposium} on {Telecommunications} ({IST})},
	author = {Farahani, Marzieh Davoodabadi and Mozayani, Nasser},
	month = dec,
	year = {2018},
	keywords = {Acceleration, Clustering algorithms, Community Detection, Detection algorithms, Hierarchical Reinforcement Learning, Image edge detection, Macro-Action Evaluation, Option, Partitioning algorithms, Reinforcement learning, Task analysis, Temporal Abstraction},
	pages = {183--186},
}

@article{davoodabadi_farahani_automatic_2019,
	title = {Automatic construction and evaluation of macro-actions in reinforcement learning},
	volume = {82},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494619303540},
	doi = {10.1016/j.asoc.2019.105574},
	abstract = {In this paper, we propose a new subgoal-based method for automatic construction of useful macro-actions modeled with option framework. We propose a new community detection algorithm to provide an appropriate partitioning of the agent’ transition graph. Subgoals are considered as the border states of the transition graph communities and options are constructed for taking the agent from one community to other communities. Despite the importance of considering the effect of each macro-action on learning speed, there is no generic known mechanism for evaluating macro-actions in the literature. We show that using all of the detected macro-actions are not useful and even in a simple environment, the augmentation of the action space with useless or wrong macro-actions may easily worsen learning performance. We propose four different heuristics for evaluating options. We identify, in this way, inappropriate options and discard them from the agent choices. Experimental results show significant improvements in the speed of learning after pruning options.},
	urldate = {2024-02-21},
	journal = {Applied Soft Computing},
	author = {Davoodabadi Farahani, Marzieh and Mozayani, Nasser},
	month = sep,
	year = {2019},
	keywords = {Community detection, Hierarchical reinforcement learning, Option evaluation, Subgoal, Temporal abstraction},
	pages = {105574},
}

@misc{durugkar_deep_2016,
	title = {Deep {Reinforcement} {Learning} {With} {Macro}-{Actions}},
	url = {http://arxiv.org/abs/1606.04615},
	doi = {10.48550/arXiv.1606.04615},
	abstract = {Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Durugkar, Ishan P. and Rosenbaum, Clemens and Dernbach, Stefan and Mahadevan, Sridhar},
	month = jun,
	year = {2016},
	note = {arXiv:1606.04615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{tessler_deep_2017,
	title = {A {Deep} {Hierarchical} {Approach} to {Lifelong} {Learning} in {Minecraft}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10744},
	doi = {10.1609/aaai.v31i1.10744},
	abstract = {We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.},
	language = {en},
	number = {1},
	urldate = {2024-02-21},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {Skills},
}

@misc{gregor_variational_2016,
	title = {Variational {Intrinsic} {Control}},
	url = {http://arxiv.org/abs/1611.07507},
	doi = {10.48550/arXiv.1611.07507},
	abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07507 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{pateria_hierarchical_2021,
	title = {Hierarchical {Reinforcement} {Learning}: {A} {Comprehensive} {Survey}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Hierarchical {Reinforcement} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3453160},
	doi = {10.1145/3453160},
	abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
	number = {5},
	urldate = {2024-02-21},
	journal = {ACM Computing Surveys},
	author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
	month = jun,
	year = {2021},
	keywords = {Hierarchical reinforcement learning, hierarchical reinforcement learning survey, hierarchical reinforcement learning taxonomy, skill discovery, subtask discovery},
	pages = {109:1--109:35},
}

@misc{omidshafiei_decentralized_2015,
	title = {Decentralized {Control} of {Partially} {Observable} {Markov} {Decision} {Processes} using {Belief} {Space} {Macro}-actions},
	url = {http://arxiv.org/abs/1502.06030},
	doi = {10.48550/arXiv.1502.06030},
	abstract = {The focus of this paper is on solving multi-robot planning problems in continuous spaces with partial observability. Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for multi-robot coordination problems, but representing and solving Dec-POMDPs is often intractable for large problems. To allow for a high-level representation that is natural for multi-robot problems and scalable to large discrete and continuous problems, this paper extends the Dec-POMDP model to the decentralized partially observable semi-Markov decision process (Dec-POSMDP). The Dec-POSMDP formulation allows asynchronous decision-making by the robots, which is crucial in multi-robot domains. We also present an algorithm for solving this Dec-POSMDP which is much more scalable than previous methods since it can incorporate closed-loop belief space macro-actions in planning. These macro-actions are automatically constructed to produce robust solutions. The proposed method's performance is evaluated on a complex multi-robot package delivery problem under uncertainty, showing that our approach can naturally represent multi-robot problems and provide high-quality solutions for large-scale problems.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar and Amato, Christopher and How, Jonathan P.},
	month = feb,
	year = {2015},
	note = {arXiv:1502.06030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{ramesh_successor_2019,
	title = {Successor {Options}: {An} {Option} {Discovery} {Framework} for {Reinforcement} {Learning}},
	shorttitle = {Successor {Options}},
	url = {http://arxiv.org/abs/1905.05731},
	doi = {10.48550/arXiv.1905.05731},
	abstract = {The options framework in reinforcement learning models the notion of a skill or a temporally extended sequence of actions. The discovery of a reusable set of skills has typically entailed building options, that navigate to bottleneck states. This work adopts a complementary approach, where we attempt to discover options that navigate to landmark states. These states are prototypical representatives of well-connected regions and can hence access the associated region with relative ease. In this work, we propose Successor Options, which leverages Successor Representations to build a model of the state space. The intra-option policies are learnt using a novel pseudo-reward and the model scales to high-dimensional spaces easily. Additionally, we also propose an Incremental Successor Options model that iterates between constructing Successor Representations and building options, which is useful when robust Successor Representations cannot be built solely from primitive actions. We demonstrate the efficacy of our approach on a collection of grid-worlds, and on the high-dimensional robotic control environment of Fetch.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Ramesh, Rahul and Tomar, Manan and Ravindran, Balaraman},
	month = may,
	year = {2019},
	note = {arXiv:1905.05731 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xiao_macro-action-based_2021,
	title = {Macro-{Action}-{Based} {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2004.08646},
	doi = {10.48550/arXiv.2004.08646},
	abstract = {In real-world multi-robot systems, performing high-quality, collaborative behaviors requires robots to asynchronously reason about high-level action selection at varying time durations. Macro-Action Decentralized Partially Observable Markov Decision Processes (MacDec-POMDPs) provide a general framework for asynchronous decision making under uncertainty in fully cooperative multi-agent tasks. However, multi-agent deep reinforcement learning methods have only been developed for (synchronous) primitive-action problems. This paper proposes two Deep Q-Network (DQN) based methods for learning decentralized and centralized macro-action-value functions with novel macro-action trajectory replay buffers introduced for each case. Evaluations on benchmark problems and a larger domain demonstrate the advantage of learning with macro-actions over primitive-actions and the scalability of our approaches.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Xiao, Yuchen and Hoffman, Joshua and Amato, Christopher},
	month = oct,
	year = {2021},
	note = {arXiv:2004.08646 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{noauthor_notitle_nodate-1,
}

@incollection{wiering_decentralized_2012,
	address = {Berlin, Heidelberg},
	title = {Decentralized {POMDPs}},
	volume = {12},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	url = {https://link.springer.com/10.1007/978-3-642-27645-3_15},
	abstract = {This chapter presents an overview of the decentralized POMDP (Dec-POMDP) framework. In a Dec-POMDP, a team of agents collaborates to maximize a global reward based on local information only. This means that agents do not observe a Markovian signal during execution and therefore the agents’ individual policies map from histories to actions. Searching for an optimal joint policy is an extremely hard problem: it is NEXP-complete. This suggests, assuming NEXP=EXP, that any optimal solution method will require doubly exponential time in the worst case. This chapter focuses on planning for Dec-POMDPs over a ﬁnite horizon. It covers the forward heuristic search approach to solving Dec-POMDPs, as well as the backward dynamic programming approach. Also, it discusses how these relate to the optimal Q-value function of a Dec-POMDP. Finally, it provides pointers to other solution methods and further related topics.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Reinforcement {Learning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Oliehoek, Frans A.},
	editor = {Wiering, Marco and Van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3_15},
	note = {Series Title: Adaptation, Learning, and Optimization},
	pages = {471--503},
}

@misc{lauri_information_2019,
	title = {Information {Gathering} in {Decentralized} {POMDPs} by {Policy} {Graph} {Improvement}},
	url = {http://arxiv.org/abs/1902.09840},
	abstract = {Decentralized policies for information gathering are required when multiple autonomous agents are deployed to collect data about a phenomenon of interest without the ability to communicate. Decentralized partially observable Markov decision processes (DecPOMDPs) are a general, principled model well-suited for such decentralized multiagent decision-making problems. In this paper, we investigate Dec-POMDPs for decentralized information gathering problems. An optimal solution of a Dec-POMDP maximizes the expected sum of rewards over time. To encourage information gathering, we set the reward as a function of the agents’ state information, for example the negative Shannon entropy. We prove that if the reward is convex, then the ﬁnite-horizon value function of the corresponding Dec-POMDP is also convex. We propose the ﬁrst heuristic algorithm for information gathering Dec-POMDPs, and empirically prove its eﬀectiveness by solving problems an order of magnitude larger than previous state-of-the-art.},
	language = {en},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Lauri, Mikko and Pajarinen, Joni and Peters, Jan},
	month = feb,
	year = {2019},
	note = {arXiv:1902.09840 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{lowe_multi-agent_2020,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	url = {http://arxiv.org/abs/1706.02275},
	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difﬁculty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
	language = {en},
	urldate = {2023-12-12},
	publisher = {arXiv},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	month = mar,
	year = {2020},
	note = {arXiv:1706.02275 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{chen_variational_nodate,
	title = {Variational {Automatic} {Curriculum} {Learning} for {Sparse}-{Reward} {Cooperative} {Multi}-{Agent} {Problems}},
	abstract = {We introduce a curriculum learning algorithm, Variational Automatic Curriculum Learning (VACL), for solving challenging goal-conditioned cooperative multiagent reinforcement learning problems. We motivate our paradigm through a variational perspective, where the learning objective can be decomposed into two terms: task learning on the current task distribution, and curriculum update to a new task distribution. Local optimization over the second term suggests that the curriculum should gradually expand the training tasks from easy to hard. Our VACL algorithm implements this variational paradigm with two practical components, task expansion and entity progression, which produces training curricula over both the task conﬁgurations as well as the number of entities in the task. Experiment results show that VACL solves a collection of sparse-reward problems with a large number of agents. Particularly, using a single desktop machine, VACL achieves 98\% coverage rate with 100 agents in the simple-spread benchmark and reproduces the ramp-use behavior originally shown in OpenAI’s hide-and-seek project. Our project website is at https://sites.google.com/view/vacl-neurips-2021.},
	language = {en},
	author = {Chen, Jiayu and Zhang, Yuanxin and Xu, Yuanfan and Ma, Huimin and Yang, Huazhong and Song, Jiaming and Wang, Yu and Wu, Yi},
}

@article{klink_self-paced_nodate,
	title = {Self-{Paced} {Deep} {Reinforcement} {Learning}},
	abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm signiﬁcantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
	language = {en},
	author = {Klink, Pascal and D’Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
}

@misc{hammond_multi-agent_2021,
	title = {Multi-{Agent} {Reinforcement} {Learning} with {Temporal} {Logic} {Specifications}},
	url = {http://arxiv.org/abs/2102.00582},
	abstract = {In this paper, we study the problem of learning to satisfy temporal logic specifications with a group of agents in an unknown environment, which may exhibit probabilistic behaviour. From a learning perspective these specifications provide a rich formal language with which to capture tasks or objectives, while from a logic and automated verification perspective the introduction of learning capabilities allows for practical applications in large, stochastic, unknown environments. The existing work in this area is, however, limited. Of the frameworks that consider full linear temporal logic or have correctness guarantees, all methods thus far consider only the case of a single temporal logic specification and a single agent. In order to overcome this limitation, we develop the first multi-agent reinforcement learning technique for temporal logic specifications, which is also novel in its ability to handle multiple specifications. We provide correctness and convergence guarantees for our main algorithm – Almanac (Automaton/Logic Multi-Agent Natural Actor-Critic) – even when using function approximation. Alongside our theoretical results, we further demonstrate the applicability of our technique via a set of preliminary experiments.},
	language = {en},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Hammond, Lewis and Abate, Alessandro and Gutierrez, Julian and Wooldridge, Michael},
	month = feb,
	year = {2021},
	note = {arXiv:2102.00582 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{yang_hierarchical_2020-1,
	title = {Hierarchical {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning} with {Skill} {Discovery}},
	url = {http://arxiv.org/abs/1912.03558},
	abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multiagent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multiagent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
	language = {en},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
	month = may,
	year = {2020},
	note = {arXiv:1912.03558 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{yang_hierarchical_2020-2,
	title = {Hierarchical {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning} with {Skill} {Discovery}},
	url = {http://arxiv.org/abs/1912.03558},
	doi = {10.48550/arXiv.1912.03558},
	abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
	month = may,
	year = {2020},
	note = {arXiv:1912.03558 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{qi_intent-aware_2018,
	title = {Intent-aware {Multi}-agent {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1803.02018},
	abstract = {This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents’ intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people’s intents will pose an inﬂuence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.},
	language = {en},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Qi, Siyuan and Zhu, Song-Chun},
	month = mar,
	year = {2018},
	note = {arXiv:1803.02018 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{qi_intent-aware_2018-1,
	title = {Intent-aware {Multi}-agent {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1803.02018},
	doi = {10.48550/arXiv.1803.02018},
	abstract = {This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents' intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people's intents will pose an influence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Qi, Siyuan and Zhu, Song-Chun},
	month = mar,
	year = {2018},
	note = {arXiv:1803.02018 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{muller_genaichi_2023,
	address = {Hamburg Germany},
	title = {{GenAICHI} 2023: {Generative} {AI} and {HCI} at {CHI} 2023},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{GenAICHI} 2023},
	url = {https://dl.acm.org/doi/10.1145/3544549.3573794},
	doi = {10.1145/3544549.3573794},
	abstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, code, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities in ethical ways. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together.},
	language = {en},
	urldate = {2023-12-06},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and Liao, Q. Vera and Maher, Mary Lou and Martin, Charles Patrick and Walsh, Greg},
	month = apr,
	year = {2023},
	pages = {1--7},
}

@inproceedings{muller_genaichi_2023-1,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {{GenAICHI} 2023: {Generative} {AI} and {HCI} at {CHI} 2023},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{GenAICHI} 2023},
	url = {https://doi.org/10.1145/3544549.3573794},
	doi = {10.1145/3544549.3573794},
	abstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, code, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities in ethical ways. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. Following a successful workshop in 2022, we convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain.},
	urldate = {2023-12-06},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and Liao, Q. Vera and Maher, Mary Lou and Martin, Charles Patrick and Walsh, Greg},
	month = apr,
	year = {2023},
	keywords = {Bias, Design, Generative AI, Uncertainty.},
	pages = {1--7},
}

@inproceedings{du_rapsai_2023,
	address = {Hamburg Germany},
	title = {Rapsai: {Accelerating} {Machine} {Learning} {Prototyping} of {Multimedia} {Applications} through {Visual} {Programming}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Rapsai},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581338},
	doi = {10.1145/3544548.3581338},
	abstract = {In recent years, there has been a proliferation of multimedia applications that leverage machine learning (ML) for interactive experiences. Prototyping ML-based applications is, however, still challenging, given complex workflows that are not ideal for design and experimentation. To better understand these challenges, we conducted a formative study with seven ML practitioners to gather insights about common ML evaluation workflows.},
	language = {en},
	urldate = {2023-12-06},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Du, Ruofei and Li, Na and Jin, Jing and Carney, Michelle and Miles, Scott and Kleiner, Maria and Yuan, Xiuxiu and Zhang, Yinda and Kulkarni, Anuva and Liu, Xingyu and Sabie, Ahmed and Orts-Escolano, Sergio and Kar, Abhishek and Yu, Ping and Iyengar, Ram and Kowdle, Adarsh and Olwal, Alex},
	month = apr,
	year = {2023},
	pages = {1--23},
}

@inproceedings{kazemitabaar_studying_2023,
	address = {Hamburg Germany},
	title = {Studying the effect of {AI} {Code} {Generators} on {Supporting} {Novice} {Learners} in {Introductory} {Programming}},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580919},
	doi = {10.1145/3544548.3580919},
	abstract = {AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modifcation task. Our results show that using Codex signifcantly increased codeauthoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual codemodifcation tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this diference did not reach statistical signifcance. Of interest, learners with higher Scratch pre-test scores performed signifcantly better on retention post-tests, if they had prior access to Codex.},
	language = {en},
	urldate = {2023-12-06},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
	month = apr,
	year = {2023},
	pages = {1--23},
}

@inproceedings{lam_model_2023,
	address = {Hamburg Germany},
	title = {Model {Sketching}: {Centering} {Concepts} in {Early}-{Stage} {Machine} {Learning} {Model} {Design}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Model {Sketching}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581290},
	doi = {10.1145/3544548.3581290},
	language = {en},
	urldate = {2023-12-06},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lam, Michelle S. and Ma, Zixian and Li, Anne and Freitas, Izequiel and Wang, Dakuo and Landay, James A. and Bernstein, Michael S.},
	month = apr,
	year = {2023},
	pages = {1--24},
}

@article{han_multiagent_2022,
	title = {Multiagent {Model}-based {Credit} {Assignment} for {Continuous} {Control}},
	abstract = {Deep reinforcement learning (RL) has recently shown great promise in robotic continuous control tasks. Nevertheless, prior research in this vein center around the centralized learning setting that largely relies on the communication availability among all the components of a robot. However, agents in the real world often operate in a decentralised fashion without communication due to latency requirements, limited power budgets and safety concerns. By formulating robotic components as a system of decentralised agents, this work presents a decentralised multiagent reinforcement learning framework for continuous control. To this end, we first develop a cooperative multiagent PPO framework that allows for centralized optimisation during training and decentralised operation during execution. However, the system only receives a global reward signal which is not attributed towards each agent. To address this challenge, we further propose a generic game-theoretic credit assignment framework which computes agent-specific reward signals. Last but not least, we also incorporate a model-based RL module into our credit assignment framework, which leads to significant improvement in sample efficiency. Finally, we empirically demonstrate the effectiveness of our framework on Mujoco locomotion control tasks.},
	language = {en},
	author = {Han, Dongge},
	year = {2022},
}

@article{han_multiagent_2022-1,
	title = {Multiagent {Model}-based {Credit} {Assignment} for {Continuous} {Control}},
	abstract = {Deep reinforcement learning (RL) has recently shown great promise in robotic continuous control tasks. Nevertheless, prior research in this vein center around the centralized learning setting that largely relies on the communication availability among all the components of a robot. However, agents in the real world often operate in a decentralised fashion without communication due to latency requirements, limited power budgets and safety concerns. By formulating robotic components as a system of decentralised agents, this work presents a decentralised multiagent reinforcement learning framework for continuous control. To this end, we first develop a cooperative multiagent PPO framework that allows for centralized optimisation during training and decentralised operation during execution. However, the system only receives a global reward signal which is not attributed towards each agent. To address this challenge, we further propose a generic game-theoretic credit assignment framework which computes agent-specific reward signals. Last but not least, we also incorporate a model-based RL module into our credit assignment framework, which leads to significant improvement in sample efficiency. Finally, we empirically demonstrate the effectiveness of our framework on Mujoco locomotion control tasks.},
	language = {en},
	author = {Han, Dongge},
	year = {2022},
}

@article{noauthor_multi-agent_nodate,
	title = {Multi-{Agent} {Common} {Knowledge} {Reinforcement} {Learning}},
	abstract = {Cooperative multi-agent reinforcement learning often requires decentralised policies, which severely limit the agents’ ability to coordinate their behaviour. In this paper, we show that common knowledge between agents allows for complex decentralised coordination. Common knowledge arises naturally in a large number of decentralised cooperative multi-agent tasks, for example, when agents can reconstruct parts of each others’ observations. Since agents can independently agree on their common knowledge, they can execute complex coordinated policies that condition on this knowledge in a fully decentralised fashion. We propose multiagent common knowledge reinforcement learning (MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical policy tree. Higher levels in the hierarchy coordinate groups of agents by conditioning on their common knowledge, or delegate to lower levels with smaller subgroups but potentially richer common knowledge. The entire policy tree can be executed in a fully decentralised fashion. As the lowest policy tree level consists of independent policies for each agent, MACKRL reduces to independently learnt decentralised policies as a special case. We demonstrate that our method can exploit common knowledge for superior performance on complex decentralised coordination tasks, including a stochastic matrix game and challenging problems in StarCraft II unit micromanagement.},
	language = {en},
}

@inproceedings{wang_popblends_2023,
	address = {Hamburg Germany},
	title = {{PopBlends}: {Strategies} for {Conceptual} {Blending} with {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {{PopBlends}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580948},
	doi = {10.1145/3544548.3580948},
	abstract = {Pop culture is an important aspect of communication. On social media people often post pop culture reference images that connect an event, product or other entity to a pop culture domain. Creating these images is a creative challenge that requires fnding a conceptual connection between the users’ topic and a pop culture domain.},
	language = {en},
	urldate = {2023-12-04},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Sitong and Petridis, Savvas and Kwon, Taeahn and Ma, Xiaojuan and Chilton, Lydia B},
	month = apr,
	year = {2023},
	pages = {1--19},
}

@inproceedings{wang_reprompt_2023,
	address = {Hamburg Germany},
	title = {{RePrompt}: {Automatic} {Prompt} {Editing} to {Refine} {AI}-{Generative} {Art} {Towards} {Precise} {Expressions}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {{RePrompt}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581402},
	doi = {10.1145/3544548.3581402},
	abstract = {Generative AI models have shown impressive ability to produce images with text prompts, which could benefit creativity in visual art creation and self-expression. However, it is unclear how precisely the generated images express contexts and emotions from the input texts. We explored the emotional expressiveness of AI-generated images and developed RePrompt, an automatic method to refine text prompts toward precise expression of the generated images. Inspired by crowdsourced editing strategies, we curated intuitive text features, such as the number and concreteness of nouns, and trained a proxy model to analyze the feature effects on the AI-generated image. With model explanations of the proxy model, we curated a rubric to adjust text prompts to optimize image generation for precise emotion expression. We conducted simulation and user studies, which showed that RePrompt significantly improves the emotional expressiveness of AI-generated images, especially for negative emotions.},
	language = {en},
	urldate = {2023-12-04},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Yunlong and Shen, Shuyuan and Lim, Brian Y},
	month = apr,
	year = {2023},
	pages = {1--29},
}

@inproceedings{zhou_case_2023,
	address = {Hamburg Germany},
	title = {A {Case} {Study} on {Scaffolding} {Exploratory} {Data} {Analysis} for {AI} {Pair} {Programmers}},
	isbn = {978-1-4503-9422-2},
	url = {https://dl.acm.org/doi/10.1145/3544549.3583943},
	doi = {10.1145/3544549.3583943},
	abstract = {Recent advances in automatic code generation have made tools like GitHub Copilot attractive for programmers, as they allow for the creation of code blocks by simply providing descriptive prompts to the AI. While researchers have studied the performance of these AI-based tools in general-purpose programming, their efectiveness in data analysis is understudied. Unlike general-purpose programming which focuses more on algorithm-driven tasks like building novel software, data analysis requires a data-driven approach to actually gain insights. It remains unclear how these tools could be utilized to help data scientists analyze real-world problems. In this paper, we conducted a qualitative user study with 5 participants to understand the use of GitHub Copilot in solving problems by scafolding prompts at diferent levels of specifcity among data scientists. We discovered that efective prompts require carefully selected terminology, properly arranged word order, and sufciently established interaction between humans and GitHub Copilot. We also spot some potential faws in GitHub Copilot that hinder data scientists from efciently scafolding prompts. Our work points out some improvement directions for both data scientists and GitHub Copilot in the future.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhou, Haoquan and Li, Jingbo},
	month = apr,
	year = {2023},
	pages = {1--7},
}

@inproceedings{mcnutt_design_2023,
	address = {Hamburg Germany},
	title = {On the {Design} of {AI}-powered {Code} {Assistants} for {Notebooks}},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580940},
	doi = {10.1145/3544548.3580940},
	abstract = {AI-powered code assistants, such as Copilot, are quickly becoming a ubiquitous component of contemporary coding contexts. Among these environments, computational notebooks, such as Jupyter, are of particular interest as they provide rich interface afordances that interleave code and output in a manner that allows for both exploratory and presentational work. Despite their popularity, little is known about the appropriate design of code assistants in notebooks. We investigate the potential of code assistants in computational notebooks by creating a design space (reifed from a survey of extant tools) and through an interview-design study (with 15 practicing data scientists). Through this work, we identify challenges and opportunities for future systems in this space, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specifc tools (like linters), and the importance of polite assistants.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Mcnutt, Andrew M and Wang, Chenglong and Deline, Robert A and Drucker, Steven M.},
	month = apr,
	year = {2023},
	pages = {1--16},
}

@inproceedings{lee_dapie_2023,
	address = {Hamburg Germany},
	title = {{DAPIE}: {Interactive} {Step}-by-{Step} {Explanatory} {Dialogues} to {Answer} {Children}’s {Why} and {How} {Questions}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {{DAPIE}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581369},
	doi = {10.1145/3544548.3581369},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lee, Yoonjoo and Kim, Tae Soo and Kim, Sungdong and Yun, Yohan and Kim, Juho},
	month = apr,
	year = {2023},
	pages = {1--22},
}

@inproceedings{jones_embodying_2023,
	address = {Hamburg Germany},
	title = {Embodying the {Algorithm}: {Exploring} {Relationships} with {Large} {Language} {Models} {Through} {Artistic} {Performance}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Embodying the {Algorithm}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580885},
	doi = {10.1145/3544548.3580885},
	abstract = {Despite the proliferation of research on how people engage with and experience algorithmic systems, the materiality and physicality of these experiences is often overlooked. We tend to forget about bodies. The Embodying the Algorithm1 project worked with artists to explore the experience of translating algorithmically produced performance instructions through human bodies. As performers interpreted the rules of engagement produced by GPT-3, they struggled with the lack of consideration the rules showed for the limits of the human body. Performers made sense of their experience through personification, reflexivity, and interpretation, which gave rise to three modes of relating with the algorithm – agonistic, perfunctory, and agreeable. We demonstrate that collaboration with algorithmic systems is ultimately impossible as people can only relate to algorithmic systems (a one-way relation) due to the material limitations of algorithmic systems for reciprocity, understanding, and consideration for the human body.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Jones, Mirabelle and Neumayer, Christina and Shklovski, Irina},
	month = apr,
	year = {2023},
	pages = {1--24},
}

@inproceedings{mirowski_co-writing_2023,
	address = {Hamburg Germany},
	title = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}: {Evaluation} by {Industry} {Professionals}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581225},
	doi = {10.1145/3544548.3581225},
	abstract = {Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron’s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations—including plagiarism and bias—and participatory models for the design and deployment of such tools.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
	month = apr,
	year = {2023},
	pages = {1--34},
}

@inproceedings{zhou_synthetic_2023,
	address = {Hamburg Germany},
	title = {Synthetic {Lies}: {Understanding} {AI}-{Generated} {Misinformation} and {Evaluating} {Algorithmic} and {Human} {Solutions}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Synthetic {Lies}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581318},
	doi = {10.1145/3544548.3581318},
	abstract = {Large language models have abilities in creating high-volume humanlike texts and can be used to generate persuasive misinformation. However, the risks remain under-explored. To address the gap, this work frst examined characteristics of AI-generated misinformation (AI-misinfo) compared with human creations, and then evaluated the applicability of existing solutions. We compiled human-created COVID-19 misinformation and abstracted it into narrative prompts for a language model to output AI-misinfo. We found signifcant linguistic diferences within human-AI pairs, and patterns of AImisinfo in enhancing details, communicating uncertainties, drawing conclusions, and simulating personal tones. While existing models remained capable of classifying AI-misinfo, a signifcant performance drop compared to human-misinfo was observed. Results suggested that existing information assessment guidelines had questionable applicability, as AI-misinfo tended to meet criteria in evidence credibility, source transparency, and limitation acknowledgment. We discuss implications for practitioners, researchers, and journalists, as AI can create new challenges to the societal problem of misinformation.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhou, Jiawei and Zhang, Yixuan and Luo, Qianni and Parker, Andrea G and De Choudhury, Munmun},
	month = apr,
	year = {2023},
	pages = {1--20},
}

@inproceedings{zhou_synthetic_2023-1,
	address = {Hamburg Germany},
	title = {Synthetic {Lies}: {Understanding} {AI}-{Generated} {Misinformation} and {Evaluating} {Algorithmic} and {Human} {Solutions}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Synthetic {Lies}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581318},
	doi = {10.1145/3544548.3581318},
	abstract = {Large language models have abilities in creating high-volume humanlike texts and can be used to generate persuasive misinformation. However, the risks remain under-explored. To address the gap, this work frst examined characteristics of AI-generated misinformation (AI-misinfo) compared with human creations, and then evaluated the applicability of existing solutions. We compiled human-created COVID-19 misinformation and abstracted it into narrative prompts for a language model to output AI-misinfo. We found signifcant linguistic diferences within human-AI pairs, and patterns of AImisinfo in enhancing details, communicating uncertainties, drawing conclusions, and simulating personal tones. While existing models remained capable of classifying AI-misinfo, a signifcant performance drop compared to human-misinfo was observed. Results suggested that existing information assessment guidelines had questionable applicability, as AI-misinfo tended to meet criteria in evidence credibility, source transparency, and limitation acknowledgment. We discuss implications for practitioners, researchers, and journalists, as AI can create new challenges to the societal problem of misinformation.},
	language = {en},
	urldate = {2023-12-01},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhou, Jiawei and Zhang, Yixuan and Luo, Qianni and Parker, Andrea G and De Choudhury, Munmun},
	month = apr,
	year = {2023},
	pages = {1--20},
}

@misc{noauthor_notitle_nodate-2,
}

@inproceedings{wang_designing_2023,
	address = {Hamburg Germany},
	title = {Designing {Responsible} {AI}: {Adaptations} of {UX} {Practice} to {Meet} {Responsible} {AI} {Challenges}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Designing {Responsible} {AI}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581278},
	doi = {10.1145/3544548.3581278},
	abstract = {Technology companies continue to invest in eforts to incorporate responsibility in their Artifcial Intelligence (AI) advancements, while eforts to audit and regulate AI systems expand. This shift towards Responsible AI (RAI) in the tech industry necessitates new practices and adaptations to roles—undertaken by a variety of practitioners in more or less formal positions, many of whom focus on the user-centered aspects of AI. To better understand practices at the intersection of user experience (UX) and RAI, we conducted an interview study with industrial UX practitioners and RAI subject matter experts, both of whom are actively involved in addressing RAI concerns throughout the early design and development of new AI-based prototypes, demos, and products, at a large technology company. Many of the specifc practices and their associated challenges have yet to be surfaced in the literature, and distilling them ofers a critical view into how practitioners’ roles are adapting to meet present-day RAI challenges. We present and discuss three emerging practices in which RAI is being enacted and reifed in UX practitioners’ everyday work. We conclude by arguing that the emerging practices, goals, and types of expertise that surfaced in our study point to an evolution in praxis, with associated challenges that suggest important areas for further research in HCI.},
	language = {en},
	urldate = {2023-12-01},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Qiaosi and Madaio, Michael and Kane, Shaun and Kapania, Shivani and Terry, Michael and Wilcox, Lauren},
	month = apr,
	year = {2023},
	pages = {1--16},
}

@misc{noauthor_designing_nodate,
	title = {Designing {Responsible} {AI}: {Adaptations} of {UX} {Practice} to {Meet} {Responsible} {AI} {Challenges} {\textbar} {Proceedings} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581278},
	urldate = {2023-12-01},
}

@inproceedings{jiang_discovering_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Discovering the {Syntax} and {Strategies} of {Natural} {Language} {Programming} with {Generative} {Language} {Models}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501870},
	doi = {10.1145/3491102.3501870},
	abstract = {In this paper, we present a natural language code synthesis tool, GenLine, backed by 1) a large generative language model and 2) a set of task-specific prompts that create or change code. To understand the user experience of natural language code synthesis with these new types of models, we conducted a user study in which participants applied GenLine to two programming tasks. Our results indicate that while natural language code synthesis can sometimes provide a magical experience, participants still faced challenges. In particular, participants felt that they needed to learn the model’s “syntax,” despite their input being natural language. Participants also struggled to form an accurate mental model of the types of requests the model can reliably translate and developed a set of strategies to debug model input. From these findings, we discuss design implications for future natural language code synthesis tools built using large generative language models.},
	urldate = {2023-12-01},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Ellen and Toh, Edwin and Molina, Alejandra and Olson, Kristen and Kayacik, Claire and Donsbach, Aaron and Cai, Carrie J and Terry, Michael},
	month = apr,
	year = {2022},
	keywords = {code synthesis, generative language models, prompt programming},
	pages = {1--19},
}

@article{noauthor_how_2018,
	title = {How {Beginning} {Programmers} and {Code} {LLMs} ({Mis})read {Each} {Other}},
	language = {en},
	year = {2018},
}

@article{blackwell_reification_2006,
	title = {The reification of metaphor as a design tool},
	volume = {13},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/1188816.1188820},
	doi = {10.1145/1188816.1188820},
	abstract = {Despite causing many debates in human-computer interaction (HCI), the term “metaphor” remains a central element of design practice. This article investigates the history of ideas behind user-interface (UI) metaphor, not only technical developments, but also less familiar perspectives from education, philosophy, and the sociology of science. The historical analysis is complemented by a study of attitudes toward metaphor among HCI researchers 30 years later. Working from these two streams of evidence, we find new insights into the way that theories in HCI are related to interface design, and offer recommendations regarding approaches to future UI design research.},
	language = {en},
	number = {4},
	urldate = {2023-11-14},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Blackwell, Alan F.},
	month = dec,
	year = {2006},
	pages = {490--530},
}

@article{hasselt_double_nodate,
	title = {Double {Q}-learning},
	abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
	language = {en},
	author = {Hasselt, Hado V},
}

@article{amato_modeling_nodate,
	title = {Modeling and {Planning} with {Macro}-{Actions} in {Decentralized} {POMDPs}},
	abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for decentralized multi-agent decision making under uncertainty. However, they typically model a problem at a low level of granularity, where each agent’s actions are primitive operations lasting exactly one time step. We address the case where each agent has macro-actions: temporally extended actions that may require diﬀerent amounts of time to execute. We model macro-actions as options in a Dec-POMDP, focusing on actions that depend only on information directly available to the agent during execution. Therefore, we model systems where coordination decisions only occur at the level of deciding which macro-actions to execute. The core technical diﬃculty in this setting is that the options chosen by each agent no longer terminate at the same time. We extend three leading DecPOMDP algorithms for policy generation to the macro-action case, and demonstrate their eﬀectiveness in both standard benchmarks and a multi-robot coordination problem. The results show that our new algorithms retain agent coordination while allowing high-quality solutions to be generated for signiﬁcantly longer horizons and larger state-spaces than previous Dec-POMDP methods. Furthermore, in the multi-robot domain, we show that, in contrast to most existing methods that are specialized to a particular problem class, our approach can synthesize control policies that exploit opportunities for coordination while balancing uncertainty, sensor information, and information about other agents.},
	language = {en},
	author = {Amato, Christopher and Konidaris, George and Kaelbling, Leslie P},
}

@misc{noauthor_multi-agent_nodate-1,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Foundations} and {Modern} {Approaches}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://www.marl-book.com/},
	abstract = {Textbook published by MIT Press},
	language = {en},
	urldate = {2023-11-06},
}

@article{zahabi_application_2019,
	title = {Application of {Cognitive} {Task} {Performance} {Modeling} for {Assessing} {Usability} of {Transradial} {Prostheses}},
	volume = {49},
	issn = {2168-2291, 2168-2305},
	url = {https://ieeexplore.ieee.org/document/8668420/},
	doi = {10.1109/THMS.2019.2903188},
	abstract = {The goal of this study was to investigate the use of cognitive modeling to assess the usability of an upper-limb prosthesis with a focus on mental workload responses. Prior studies have investigated usability of upper-limb prostheses with subjective surveys and physiological measures. However, these approaches have limitations, including subject recall of conditions and physiological response contamination by head and body movements and user speech during task performance as well as sensitivity to physical fatigue and room lighting conditions. Cognitive modeling was used to assess mental workload in use of transradial upper-limb prosthesis. A case study was conducted with a participant with upper-limb amputation using two different types of electromyography-based control schemes, including conventional direct control (DC) and pattern recognition (PR) control in order to compare cognitive model outcomes with mental workload assessment using eye-tracking measures. Cognitive models time estimates were also compared with actual task completion time results from the case study to further assess the validity of cognitive modeling as an analytical tool for evaluating upper limb prosthesis usability. Findings of both the cognitive models and case study revealed the PR mode to be more intuitive, reduce cognitive load, and increase efﬁciency in prosthetic control as compared to the DC mode. Results of the present study revealed that cognitive modeling can be used as an analytical approach for assessing upper-limb prosthetic device usability in terms of workload outcomes. Future studies should validate the present ﬁndings with more precise time estimations and a larger user sample size.},
	language = {en},
	number = {4},
	urldate = {2023-11-01},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Zahabi, Maryam and White, Melissa Mae and Zhang, Wenjuan and Winslow, Anna T. and Zhang, Fan and Huang, He and Kaber, David B.},
	month = aug,
	year = {2019},
	pages = {381--387},
}

@inproceedings{xiao_online_2019,
	address = {Montreal, QC, Canada},
	title = {Online {Planning} for {Target} {Object} {Search} in {Clutter} under {Partial} {Observability}},
	isbn = {978-1-5386-6027-0},
	url = {https://ieeexplore.ieee.org/document/8793494/},
	doi = {10.1109/ICRA.2019.8793494},
	abstract = {The problem of ﬁnding and grasping a target object in a cluttered, uncertain environment, target object search, is a common and important problem in robotics. One key challenge is the uncertainty of locating and recognizing each object in a cluttered environment due to noisy perception and occlusions. Furthermore, the uncertainty in localization makes manipulation difﬁcult and uncertain. To cope with these challenges, we formulate the target object search task as a partially observable Markov decision process (POMDP), enabling the robot to reason about perceptual and manipulation uncertainty while searching. To further address the manipulation difﬁculty, we propose Parameterized Action Partially Observable MonteCarlo Planning (PA-POMCP), an algorithm that evaluates manipulation actions by taking into account the effect of the robot’s current belief on the success of the action execution. In addition, a novel run-time initial belief generator and a state value estimator are introduced in this paper to facilitate the PAPOMCP algorithm. Our experiments show that our methods solve the target object search task in settings where simpler methods either take more object movements or fail.},
	language = {en},
	urldate = {2023-10-29},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Xiao, Yuchen and Katt, Sammie and Pas, Andreas Ten and Chen, Shengjian and Amato, Christopher},
	month = may,
	year = {2019},
	pages = {8241--8247},
}

@misc{xiao_learning_2020,
	title = {Learning {Multi}-{Robot} {Decentralized} {Macro}-{Action}-{Based} {Policies} via a {Centralized} {Q}-{Net}},
	url = {http://arxiv.org/abs/1909.08776},
	abstract = {In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difﬁculty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDecMADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving nearcentralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efﬁcient way.},
	language = {en},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Xiao, Yuchen and Hoffman, Joshua and Xia, Tian and Amato, Christopher},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08776 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{xiao_asynchronous_2022,
	title = {Asynchronous {Actor}-{Critic} for {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2209.10113},
	doi = {10.48550/arXiv.2209.10113},
	abstract = {Synchronizing decisions across multiple agents in realistic settings is problematic since it requires agents to wait for other agents to terminate and communicate about termination reliably. Ideally, agents should learn and execute asynchronously instead. Such asynchronous methods also allow temporally extended actions that can take different amounts of time based on the situation and action executed. Unfortunately, current policy gradient methods are not applicable in asynchronous settings, as they assume that agents synchronously reason about action selection at every time step. To allow asynchronous learning and decision-making, we formulate a set of asynchronous multi-agent actor-critic methods that allow agents to directly optimize asynchronous policies in three standard training paradigms: decentralized learning, centralized learning, and centralized training for decentralized execution. Empirical results (in simulation and hardware) in a variety of realistic domains demonstrate the superiority of our approaches in large multi-agent problems and validate the effectiveness of our algorithms for learning high-quality and asynchronous solutions.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Xiao, Yuchen and Tan, Weihao and Amato, Christopher},
	month = oct,
	year = {2022},
	note = {arXiv:2209.10113 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{lyu_deeper_2022,
	title = {A {Deeper} {Understanding} of {State}-{Based} {Critics} in {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2201.01221},
	doi = {10.48550/arXiv.2201.01221},
	abstract = {Centralized Training for Decentralized Execution, where training is done in a centralized offline fashion, has become a popular solution paradigm in Multi-Agent Reinforcement Learning. Many such methods take the form of actor-critic with state-based critics, since centralized training allows access to the true system state, which can be useful during training despite not being available at execution time. State-based critics have become a common empirical choice, albeit one which has had limited theoretical justification or analysis. In this paper, we show that state-based critics can introduce bias in the policy gradient estimates, potentially undermining the asymptotic guarantees of the algorithm. We also show that, even if the state-based critics do not introduce any bias, they can still result in a larger gradient variance, contrary to the common intuition. Finally, we show the effects of the theories in practice by comparing different forms of centralized critics on a wide range of common benchmarks, and detail how various environmental properties are related to the effectiveness of different types of critics.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Lyu, Xueguang and Baisero, Andrea and Xiao, Yuchen and Amato, Christopher},
	month = may,
	year = {2022},
	note = {arXiv:2201.01221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{petersen_differentiable_2022,
	title = {Differentiable {Top}-k {Classification} {Learning}},
	url = {http://arxiv.org/abs/2206.07290},
	doi = {10.48550/arXiv.2206.07290},
	abstract = {The top-k classification accuracy is one of the core metrics in machine learning. Here, k is conventionally a positive integer, such as 1 or 5, leading to top-1 or top-5 training objectives. In this work, we relax this assumption and optimize the model for multiple k simultaneously instead of using a single k. Leveraging recent advances in differentiable sorting and ranking, we propose a differentiable top-k cross-entropy classification loss. This allows training the network while not only considering the top-1 prediction, but also, e.g., the top-2 and top-5 predictions. We evaluate the proposed loss function for fine-tuning on state-of-the-art architectures, as well as for training from scratch. We find that relaxing k does not only produce better top-5 accuracies, but also leads to top-1 accuracy improvements. When fine-tuning publicly available ImageNet models, we achieve a new state-of-the-art for these models.},
	urldate = {2023-10-28},
	publisher = {arXiv},
	author = {Petersen, Felix and Kuehne, Hilde and Borgelt, Christian and Deussen, Oliver},
	month = jun,
	year = {2022},
	note = {arXiv:2206.07290 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{cordonnier_differentiable_2021,
	title = {Differentiable {Patch} {Selection} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Cordonnier_Differentiable_Patch_Selection_for_Image_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-10-28},
	author = {Cordonnier, Jean-Baptiste and Mahendran, Aravindh and Dosovitskiy, Alexey and Weissenborn, Dirk and Uszkoreit, Jakob and Unterthiner, Thomas},
	year = {2021},
	pages = {2351--2360},
}

@misc{kumar_introspective_2023,
	title = {Introspective {Experience} {Replay}: {Look} {Back} {When} {Surprised}},
	shorttitle = {Introspective {Experience} {Replay}},
	url = {http://arxiv.org/abs/2206.03171},
	doi = {10.48550/arXiv.2206.03171},
	abstract = {In reinforcement learning (RL), experience replay-based sampling techniques play a crucial role in promoting convergence by eliminating spurious correlations. However, widely used methods such as uniform experience replay (UER) and prioritized experience replay (PER) have been shown to have sub-optimal convergence and high seed sensitivity respectively. To address these issues, we propose a novel approach called IntrospectiveExperience Replay (IER) that selectively samples batches of data points prior to surprising events. Our method builds upon the theoretically sound reverse experience replay (RER) technique, which has been shown to reduce bias in the output of Q-learning-type algorithms with linear function approximation. However, this approach is not always practical or reliable when using neural function approximation. Through empirical evaluations, we demonstrate that IER with neural function approximation yields reliable and superior performance compared toUER, PER, and hindsight experience replay (HER) across most tasks.},
	urldate = {2023-10-28},
	publisher = {arXiv},
	author = {Kumar, Ramnath and Nagaraj, Dheeraj},
	month = feb,
	year = {2023},
	note = {arXiv:2206.03171 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ni_when_2023,
	title = {When {Do} {Transformers} {Shine} in {RL}? {Decoupling} {Memory} from {Credit} {Assignment}},
	shorttitle = {When {Do} {Transformers} {Shine} in {RL}?},
	url = {https://arxiv.org/abs/2307.03864v3},
	abstract = {Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The Transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capability of RL algorithms, scaling up to tasks that require memorizing observations \$1500\$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our results provide an explanation for the success of Transformers in RL, while also highlighting an important area for future research and benchmark design. Our code is open-sourced at https://github.com/twni2016/Memory-RL},
	language = {en},
	urldate = {2023-10-16},
	journal = {arXiv.org},
	author = {Ni, Tianwei and Ma, Michel and Eysenbach, Benjamin and Bacon, Pierre-Luc},
	month = jul,
	year = {2023},
}

@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
}

@article{sander_fast_nodate,
	title = {Fast, {Differentiable} and {Sparse} {Top}-k: a {Convex} {Analysis} {Perspective}},
	abstract = {The top-k operator returns a sparse vector, where the non-zero values correspond to the k largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-k operators. We view the top-k operator as a linear program over the permutahedron, the convex hull of permutations. We then introduce a p-norm regularization term to smooth out the operator, and show that its computation can be reduced to isotonic optimization. Our framework is significantly more general than the existing one and allows for example to express top-k operators that select values in magnitude. On the algorithmic side, in addition to pool adjacent violator (PAV) algorithms, we propose a new GPU/TPU-friendly Dykstra algorithm to solve isotonic optimization problems. We successfully use our operators to prune weights in neural networks, to fine-tune vision transformers, and as a router in sparse mixture of experts.},
	language = {en},
	author = {Sander, Michael E and Puigcerver, Joan and Djolonga, Josip and Peyre, Gabriel and Blondel, Mathieu},
}

@article{xie_differentiable_nodate,
	title = {Differentiable {Top}-k with {Optimal} {Transport}},
	abstract = {The top-k operation, i.e., ﬁnding the k largest or smallest elements from a collection of scores, is an important model component, which is widely used in information retrieval, machine learning, and data mining. However, if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator. Speciﬁcally, our SOFT top-k operator approximates the output of the top-k operation as the solution of an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can then be efﬁciently approximated based on the optimality conditions of EOT problem. We apply the proposed operator to the k-nearest neighbors and beam search algorithms, and demonstrate improved performance.},
	language = {en},
	author = {Xie, Yujia and Dai, Hanjun and Chen, Minshuo and Dai, Bo and Zhao, Tuo and Zha, Hongyuan and Wei, Wei and Pﬁster, Tomas},
}

@inproceedings{tang_neuroevolution_2020,
	title = {Neuroevolution of {Self}-{Interpretable} {Agents}},
	url = {http://arxiv.org/abs/2003.08165},
	doi = {10.1145/3377930.3389847},
	abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, nondifferentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail.},
	language = {en},
	urldate = {2023-10-09},
	booktitle = {Proceedings of the 2020 {Genetic} and {Evolutionary} {Computation} {Conference}},
	author = {Tang, Yujin and Nguyen, Duong and Ha, David},
	month = jun,
	year = {2020},
	note = {arXiv:2003.08165 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {414--424},
}

@misc{tang_neuroevolution_2020-1,
	title = {Neuroevolution of {Self}-{Interpretable} {Agents}},
	url = {https://arxiv.org/abs/2003.08165v2},
	abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Tang, Yujin and Nguyen, Duong and Ha, David},
	month = mar,
	year = {2020},
	doi = {10.1145/3377930.3389847},
}

@misc{noauthor_notitle_nodate-3,
}

@inproceedings{matthies_reflexive_2019,
	address = {Fremantle WA Australia},
	title = {Reflexive {Interaction}: {Extending} the concept of {Peripheral} {Interaction}},
	isbn = {978-1-4503-7696-9},
	shorttitle = {Reflexive {Interaction}},
	url = {https://dl.acm.org/doi/10.1145/3369457.3369478},
	doi = {10.1145/3369457.3369478},
	abstract = {Human-computer interaction (HCI) continues to evolve and interaction scenarios have to fulfill mobility, flexibility, and ad-hoc interaction where ever users are. To address this, traditional interaction concepts are being extended. While Peripheral Interaction was previously introduced, it still remains as a rather broad concept, intersecting with others, and thus creating space for further definitions. Therefore, this paper introduces the concept of Reflexive Interaction, which can be viewed as a specific manifestation of Peripheral Interaction. In contrast, Reflexive Interaction is envisioned to be executed at a secondary task without involving substantial cognitive effort. It allows the user to perform very short interactions, shorter than Microinteractions, without straining the user’s main interaction channels occupied with the primary task. To clearly classify Reflexive Interaction in respect to previous interaction concepts, we use a taxonomy relying on an attention-based HCI model.},
	language = {en},
	urldate = {2023-10-04},
	booktitle = {Proceedings of the 31st {Australian} {Conference} on {Human}-{Computer}-{Interaction}},
	publisher = {ACM},
	author = {Matthies, Denys J. C. and Urban, Bodo and Wolf, Katrin and Schmidt, Albrecht},
	month = dec,
	year = {2019},
	pages = {266--278},
}

@article{brosnan_gestalt_2004,
	title = {Gestalt processing in autism: failure to process perceptual relationships and the implications for contextual understanding},
	volume = {45},
	issn = {0021-9630, 1469-7610},
	shorttitle = {Gestalt processing in autism},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1469-7610.2004.00237.x},
	doi = {10.1111/j.1469-7610.2004.00237.x},
	language = {en},
	number = {3},
	urldate = {2023-10-04},
	journal = {Journal of Child Psychology and Psychiatry},
	author = {Brosnan, Mark J. and Scott, Fiona J. and Fox, Simone and Pye, Jackie},
	month = mar,
	year = {2004},
	pages = {459--469},
}

@misc{noauthor_files_nodate,
	title = {Files},
	url = {https://northeastern.instructure.com/courses/156966/files?preview=23515560},
	urldate = {2023-10-04},
}

@article{wang_learning_nodate,
	title = {Learning {Belief} {Representations} for {Partially} {Observable} {Deep} {RL}},
	abstract = {Many important real-world Reinforcement Learning (RL) problems involve partial observability and require policies with memory. Unfortunately, standard deep RL algorithms for partially observable settings typically condition on the full history of interactions and are notoriously difficult to train. We propose a novel deep, partially observable RL algorithm based on modelling belief states — a technique typically used when solving tabular POMDPs, but that has traditionally been difficult to apply to more complex environments. Our approach simplifies policy learning by leveraging state information at training time, that may not be available at deployment time. We do so in two ways: first, we decouple belief state modelling (via unsupervised learning) from policy optimization (via RL); and second, we propose a representation learning approach to capture a compact set of reward-relevant features of the state. Experiments demonstrate the efficacy of our approach on partially observable domains requiring information seeking and long-term memory.},
	language = {en},
	author = {Wang, Andrew and Li, Andrew C and Klassen, Toryn Q and Icarte, Rodrigo Toro and McIlraith, Sheila A},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@book{noauthor_notitle_nodate-4,
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	doi = {10.48550/arXiv.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{hausknecht_deep_2017,
	title = {Deep {Recurrent} {Q}-{Learning} for {Partially} {Observable} {MDPs}},
	url = {http://arxiv.org/abs/1507.06527},
	doi = {10.48550/arXiv.1507.06527},
	abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting {\textbackslash}textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Hausknecht, Matthew and Stone, Peter},
	month = jan,
	year = {2017},
	note = {arXiv:1507.06527 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_notitle_nodate,
}

@book{noauthor_personas_nodate,
	title = {Personas},
	url = {https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/personas},
	abstract = {Introduction to Personas and how to use Personas when designing},
	language = {en},
	urldate = {2023-09-17},
}

@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPs) and partially observable MDPs (POMDPs). We then outline a novel algorithm for solving POMDPs off line and show how, in some cases, a ﬁnite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of ﬁnding exact solutions to POMDPs, and of some possibilities for ﬁnding approximate solutions. © 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2023-09-15},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	pages = {99--134},
}

@book{noauthor_notitle_nodate-1,
}

@misc{ni_when_2023,
	title = {When {Do} {Transformers} {Shine} in {RL}? {Decoupling} {Memory} from {Credit} {Assignment}},
	shorttitle = {When {Do} {Transformers} {Shine} in {RL}?},
	url = {http://arxiv.org/abs/2307.03864},
	abstract = {Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations 1500 steps ago. However, Transformers do not improve long-term credit assignment. In summary, our results provide an explanation for the success of Transformers in RL, while also highlighting an important area for future research and benchmark design. Our code is open-sourced1.},
	language = {en},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Ni, Tianwei and Ma, Michel and Eysenbach, Benjamin and Bacon, Pierre-Luc},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03864 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@incollection{lazar_interviews_2017,
	title = {Interviews and focus groups},
	isbn = {978-0-12-805390-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B978012805390400008X},
	language = {en},
	urldate = {2023-09-13},
	booktitle = {Research {Methods} in {Human} {Computer} {Interaction}},
	publisher = {Elsevier},
	author = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	year = {2017},
	doi = {10.1016/B978-0-12-805390-4.00008-X},
	pages = {187--228},
}

@incollection{lazar_ethnography_2017,
	title = {Ethnography},
	isbn = {978-0-12-805390-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128053904000091},
	language = {en},
	urldate = {2023-09-13},
	booktitle = {Research {Methods} in {Human} {Computer} {Interaction}},
	publisher = {Elsevier},
	author = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	year = {2017},
	doi = {10.1016/B978-0-12-805390-4.00009-1},
	pages = {229--261},
}